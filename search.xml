<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>MapReduce常见问题与解决-PipeMapRed.waitOutputThreads</title>
    <url>/2019/11/13/MapReduceError-PipeMapRedWaitOutputThreadsSubprocessFailedwithCodeX/</url>
    <content><![CDATA[<h2 id="1-问题描述"><a href="#1-问题描述" class="headerlink" title="1 问题描述"></a>1 问题描述</h2><p>在Map阶段错误日志为：</p>
<pre><code>Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1 at 
org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:322) at 
org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:537) at 
org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:132) at 
org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:75) at 
org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34) at 
org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:459) at 
org.apache.hadoop.mapred.MapTask.run(MapTask.java:345) at 
org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:170) at 
java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:415) at 
org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657) at 
org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:164)</code></pre><h2 id="2-问题原因及解决"><a href="#2-问题原因及解决" class="headerlink" title="2 问题原因及解决"></a>2 问题原因及解决</h2><p>解决此类原因主是 Streaming 程序退出异常，序要依靠错误日志来查看详细原因，下面给出部分错误码</p>
<h3 id="2-1-subprocess-failed-with-code-1"><a href="#2-1-subprocess-failed-with-code-1" class="headerlink" title="2.1 subprocess failed with code 1"></a>2.1 subprocess failed with code <code>1</code></h3><p><strong>原因定位</strong>: </p>
<blockquote>
<p>map或reduce返回<code>1</code>，<code>MapReduce</code>框架会将应用程序的返回值收集，subprocess failed with code 1表示程序返回的就是1。详细出错原因需结合Task Logs和程序源码定位。</p>
</blockquote>
<h3 id="2-2-subprocess-failed-with-code-126"><a href="#2-2-subprocess-failed-with-code-126" class="headerlink" title="2.2 subprocess failed with code 126"></a>2.2 subprocess failed with code <code>126</code></h3><p><strong>原因定位</strong>: </p>
<blockquote>
<p>一般是由于map/reduce中调用的程序无可执行权限导致</p>
</blockquote>
<h3 id="2-3-subprocess-failed-with-code-127"><a href="#2-3-subprocess-failed-with-code-127" class="headerlink" title="2.3 subprocess failed with code 127"></a>2.3 subprocess failed with code <code>127</code></h3><p><strong>原因定位</strong>:</p>
<blockquote>
<p>一般是由于map/reduce脚本未上传导致，请使用-file 上传map/reduce程序</p>
</blockquote>
<h3 id="2-4-subprocess-failed-with-code-137"><a href="#2-4-subprocess-failed-with-code-137" class="headerlink" title="2.4 subprocess failed with code 137"></a>2.4 subprocess failed with code <code>137</code></h3><p><strong>原因定位</strong>:</p>
<blockquote>
<p>map或reduce程序超出平台内存限制被limit杀掉，一般的平台都会有一个默认内存限制，例如配置内存限制为800MB（137-128=9, 对应信号为SIGKILL）发生这种情况后一般登录到这台计算节点上看dmesg都能看到类似： killer: killing process….提示 ps：在新的haoop版本中，如果单个程序运行占用cpu的时间超过12小时，也会被kill返回137.</p>
</blockquote>
<h3 id="2-5-subprocess-failed-with-code-139"><a href="#2-5-subprocess-failed-with-code-139" class="headerlink" title="2.5 subprocess failed with code 139"></a>2.5 subprocess failed with code <code>139</code></h3><p><strong>原因定位</strong>:</p>
<blockquote>
<p>代码有问题，进行本地数据测试</p>
</blockquote>
<h3 id="2-6-subprocess-failed-with-code-141"><a href="#2-6-subprocess-failed-with-code-141" class="headerlink" title="2.6 subprocess failed with code 141"></a>2.6 subprocess failed with code <code>141</code></h3><p><strong>原因定位</strong>:</p>
<blockquote>
<p>map或reduce异常退出，平台继续向管道推送数据，因管道异常出错(141-128=13， 信号13代表着SIGPIPE错误，即管道错误)根本原因还是程序异常退出导致。详细出错原因需结合Task Logs和程序源码定位。</p>
</blockquote>
<h3 id="2-7-subprocess-failed-with-code-255"><a href="#2-7-subprocess-failed-with-code-255" class="headerlink" title="2.7 subprocess failed with code 255"></a>2.7 subprocess failed with code <code>255</code></h3><p><strong>原因定位</strong>:</p>
<blockquote>
<p>map或reduce异常退出返回值-1，详细出错原因需结合Task Logs和程序源码定位</p>
</blockquote>
<h3 id="2-8-其他返回值"><a href="#2-8-其他返回值" class="headerlink" title="2.8 其他返回值"></a>2.8 其他返回值</h3><p>公式：</p>
<blockquote>
<p>错误码-128 = linux返回码 </p>
<p>//比如 返回错误码是 137， 那么它对应的linux返回码就是 9</p>
</blockquote>
<p>然后查看<code>linux kill -l</code>信号含义，对应检查程序</p>
<h2 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h2><p><a href="https://hadoop.apache.org/docs/r1.2.1/streaming.html" target="_blank" rel="noopener">Hadoop Streaming Doc</a></p>
]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>MapReduce</tag>
      </tags>
  </entry>
  <entry>
    <title>SCL 管理 CentOS 软件</title>
    <url>/2019/11/08/ManageCentosPackageswithSCL/</url>
    <content><![CDATA[<h2 id="1-SCL-介绍"><a href="#1-SCL-介绍" class="headerlink" title="1 SCL 介绍"></a>1 SCL 介绍</h2><p>在管理一些机器的时候，各种软件环境的安全切换是比较麻烦的问题，有时候环境的来回切换会导致系统紊乱，为了解决这个问题，红帽提供了软件环境管理工具<code>SCL</code>(Software Collections) </p>
<p>项目主页：<a href="https://www.softwarecollections.org/en/" target="_blank" rel="noopener">Software Collections</a></p>
<p>其中包含的软件集：<a href="https://www.softwarecollections.org/en/scls/" target="_blank" rel="noopener">Directory - Software Collections</a></p>
<p>下面将以安装<code>python36</code>来作为示例</p>
<h2 id="2-SCL-安装和使用"><a href="#2-SCL-安装和使用" class="headerlink" title="2 SCL 安装和使用"></a>2 SCL 安装和使用</h2><h3 id="2-1-SCL-安装"><a href="#2-1-SCL-安装" class="headerlink" title="2.1 SCL 安装"></a>2.1 SCL 安装</h3><p>使用<code>yum</code>命令安装<code>SCL</code></p>
<pre><code>[username@centos ~] yum install centos-release-scl</code></pre><h3 id="2-2-安装需要的软件包"><a href="#2-2-安装需要的软件包" class="headerlink" title="2.2 安装需要的软件包"></a>2.2 安装需要的软件包</h3><p>示例安装<code>python36</code></p>
<pre><code>[username@centos ~] yum install rh-python36</code></pre><p>激活<code>python36</code> 软件环境</p>
<pre><code>[username@centos ~] scl enable rh-python36 bash</code></pre><p>当然也可以安装其他软件包，只要是<code>rh-</code>开头的都是被<code>SCL</code>工具管理环境，我们可以使用下面的命令来查看支持的环境，当然也可以去<a href="https://www.softwarecollections.org/en/scls/" target="_blank" rel="noopener">官网软件集</a>查看</p>
<pre><code>[username@centos ~] yum list  rh-*</code></pre><h3 id="2-3-设置默认环境"><a href="#2-3-设置默认环境" class="headerlink" title="2.3 设置默认环境"></a>2.3 设置默认环境</h3><p>在实际工作中，为了避免每次输入命令激活环境，我们都会将其写入环境变量来默认激活，可以将激活环境的 bash 加入 <code>~/.bashrc</code> 或者 <code>~/.bash_profile</code> 中, 将下列命令追加在文件最后</p>
<pre><code>source scl_source enable rh-python36</code></pre>]]></content>
      <categories>
        <category>SRE 运维</category>
      </categories>
      <tags>
        <tag>SCL</tag>
      </tags>
  </entry>
  <entry>
    <title>Latex 常用数学符号和语法索引表</title>
    <url>/2019/10/30/IndexofLatexCommonCodeandCommand/</url>
    <content><![CDATA[<h2 id="0-前言"><a href="#0-前言" class="headerlink" title="0 前言"></a>0 前言</h2><p>由于经常使用<code>Latex</code>编写数学公式，查找复制也比较麻烦，所以做么一个索引表，方便使用</p>
<p>本文持续更新</p>
<p>推荐非常好用的 手写字符识别 web 程序：<a href="http://detexify.kirelabs.org/classify.html" target="_blank" rel="noopener">http://detexify.kirelabs.org/classify.html</a> 效果如图所示：<br><img src="latex-classify.png" alt></p>
<p><strong>主要参考文档：</strong></p>
<ol>
<li><a href="http://mohu.org/info/symbols/symbols.htm" target="_blank" rel="noopener">http://mohu.org/info/symbols/symbols.htm</a></li>
</ol>
<h2 id="常用微分"><a href="#常用微分" class="headerlink" title="常用微分"></a>常用微分</h2><table>
<thead>
<tr>
<th>语法</th>
<th>效果</th>
</tr>
</thead>
<tbody><tr>
<td><code>\nabla</code></td>
<td>$\nabla$</td>
</tr>
<tr>
<td>\partial x</td>
<td>$\partial x$</td>
</tr>
<tr>
<td>\mathrm{d}x</td>
<td>$\mathrm{d}x$</td>
</tr>
<tr>
<td>\dot x</td>
<td>$\dot x$</td>
</tr>
<tr>
<td>\ddot y</td>
<td>$\ddot y$</td>
</tr>
</tbody></table>
<h2 id="常用函数"><a href="#常用函数" class="headerlink" title="常用函数"></a>常用函数</h2><table>
<thead>
<tr>
<th>语法</th>
<th>效果</th>
</tr>
</thead>
<tbody><tr>
<td>\sin\theta</td>
<td>$\sin\theta$</td>
</tr>
<tr>
<td>\cos\theta</td>
<td>$\cos\theta$</td>
</tr>
<tr>
<td>\tan\theta</td>
<td>$\tan\theta$</td>
</tr>
<tr>
<td>\max H</td>
<td>$\max H$</td>
</tr>
<tr>
<td>\min L</td>
<td>$\min L$</td>
</tr>
<tr>
<td>\exp t</td>
<td>$\exp t$</td>
</tr>
<tr>
<td>\ln X</td>
<td>$\ln X$</td>
</tr>
<tr>
<td>\lg X</td>
<td>$\lg X$</td>
</tr>
<tr>
<td>\log X</td>
<td>$\log X$</td>
</tr>
<tr>
<td>\log_\alpha X</td>
<td>$    \log_\alpha X$</td>
</tr>
<tr>
<td>\arg x</td>
<td>$\arg x$</td>
</tr>
<tr>
<td>\dim x</td>
<td>$\dim x$</td>
</tr>
</tbody></table>
<h2 id="常用希腊字母"><a href="#常用希腊字母" class="headerlink" title="常用希腊字母"></a>常用希腊字母</h2><table>
<thead>
<tr>
<th>语法</th>
<th>效果</th>
</tr>
</thead>
<tbody><tr>
<td>\alpha \beta \gamma \delta \epsilon \zeta \eta\theta</td>
<td>$\alpha \beta \gamma \delta \epsilon \zeta \eta\theta$</td>
</tr>
<tr>
<td>\iota \kappa\varkappa \lambda \mu \nu \xi \omicron\pi</td>
<td>$\iota \kappa\varkappa \lambda \mu \nu \xi \omicron\pi$</td>
</tr>
<tr>
<td>\rho \sigma \tau \upsilon \phi \chi \psi\omega</td>
<td>$\rho \sigma \tau \upsilon \phi \chi \psi\omega$</td>
</tr>
</tbody></table>
<h2 id="常用积分求和公式混合"><a href="#常用积分求和公式混合" class="headerlink" title="常用积分求和公式混合"></a>常用积分求和公式混合</h2><table>
<thead>
<tr>
<th>语法</th>
<th>效果</th>
</tr>
</thead>
<tbody><tr>
<td><code>\sum_{k=1}^N k^2</code></td>
<td>$\sum_{k=1}^N k^2$</td>
</tr>
<tr>
<td><code>\prod_{i=1}^N x_i</code></td>
<td>$\prod_{i=1}^N x_i$</td>
</tr>
<tr>
<td><code>\coprod_{i=1}^N x_i</code></td>
<td>$\coprod_{i=1}^N x_i$</td>
</tr>
<tr>
<td><code>\lim_{n \to \infty}x_n</code></td>
<td>$\lim_{n \to \infty}x_n$</td>
</tr>
<tr>
<td><code>\int_{-N}^{N} e^x\, dx</code></td>
<td>$\int_{-N}^{N} e^x\, dx$</td>
</tr>
<tr>
<td><code>\iint_{D}^{W} \, dx\,dy</code></td>
<td>$    \iint_{D}^{W} \, dx\,dy$</td>
</tr>
<tr>
<td><code>\iiint_{E}^{V} \, dx\,dy\,dz</code></td>
<td>$\iiint_{E}^{V} \, dx\,dy\,dz$</td>
</tr>
<tr>
<td><code>\iiiint_{F}^{U} \, dx\,dy\,dz\,dt</code></td>
<td>$\iiiint_{F}^{U} \, dx\,dy\,dz\,dt$</td>
</tr>
<tr>
<td><code>\oint_{C} x^3\, dx + 4y^2\, dy</code></td>
<td>$\oint_{C} x^3\, dx + 4y^2\, dy$</td>
</tr>
<tr>
<td><code>\bigcap_1^{n} p</code></td>
<td>$\bigcap_1^{n} p$</td>
</tr>
<tr>
<td><code>\bigcup_1^{k} p</code></td>
<td>$\bigcup_1^{k} p$</td>
</tr>
</tbody></table>
<h2 id="常用矩阵混合"><a href="#常用矩阵混合" class="headerlink" title="常用矩阵混合"></a>常用矩阵混合</h2><table>
<thead>
<tr>
<th>语法</th>
<th>效果</th>
</tr>
</thead>
<tbody><tr>
<td><code>\begin{matrix}x &amp; y \\ z &amp; v \end{matrix}</code></td>
<td>$\begin{matrix}x &amp; y \\ z &amp; v \end{matrix}$</td>
</tr>
<tr>
<td><code>\begin{vmatrix} x &amp; y \\z &amp; v \end{vmatrix}</code></td>
<td>$\begin{vmatrix} x &amp; y \\z &amp; v \end{vmatrix}$</td>
</tr>
<tr>
<td><code>\begin{Vmatrix} x &amp; y \\ z &amp; v \end{Vmatrix}</code></td>
<td>$\begin{Vmatrix} x &amp; y \\ z &amp; v \end{Vmatrix}$</td>
</tr>
<tr>
<td><code>\begin{bmatrix} 0      &amp; \cdots &amp; 0      \\ \vdots &amp; \ddots &amp; \vdots \\ 0      &amp; \cdots &amp; 0 \end{bmatrix}</code></td>
<td>$\begin{bmatrix} 0      &amp; \cdots &amp; 0      \\ \vdots &amp; \ddots &amp; \vdots \\ 0      &amp; \cdots &amp; 0 \end{bmatrix}$</td>
</tr>
<tr>
<td><code>\begin{Bmatrix} x &amp; y \\ z &amp; v \end{Bmatrix}</code></td>
<td>$\begin{Bmatrix} x &amp; y \\ z &amp; v \end{Bmatrix}$</td>
</tr>
<tr>
<td><code>\begin{pmatrix} x &amp; y \\ z &amp; v \end{pmatrix}</code></td>
<td>$\begin{pmatrix} x &amp; y \\ z &amp; v \end{pmatrix}$</td>
</tr>
<tr>
<td><code>\bigl( \begin{smallmatrix} a&amp;b\\ c&amp;d \end{smallmatrix} \bigr)</code></td>
<td>$\bigl( \begin{smallmatrix} a&amp;b\\ c&amp;d \end{smallmatrix} \bigr)$</td>
</tr>
</tbody></table>
]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>Latex</tag>
      </tags>
  </entry>
  <entry>
    <title>pip conda 安装速度慢解决方法</title>
    <url>/2019/10/30/SolutionofPipandCondaSlowlyInstall/</url>
    <content><![CDATA[<h2 id="1-pip-提速"><a href="#1-pip-提速" class="headerlink" title="1 pip 提速"></a>1 pip 提速</h2><h3 id="1-1-临时提速"><a href="#1-1-临时提速" class="headerlink" title="1.1 临时提速"></a>1.1 临时提速</h3><p>在<code>pip</code>命令中添加参数 <code>-i</code> 和<code>镜像</code><br>常用镜像如下：</p>
<ul>
<li>清华：<a href="https://pypi.tuna.tsinghua.edu.cn/simple" target="_blank" rel="noopener">https://pypi.tuna.tsinghua.edu.cn/simple</a></li>
<li>阿里云：<a href="http://mirrors.aliyun.com/pypi/simple/" target="_blank" rel="noopener">http://mirrors.aliyun.com/pypi/simple/</a></li>
<li>中国科技大学 <a href="https://pypi.mirrors.ustc.edu.cn/simple/" target="_blank" rel="noopener">https://pypi.mirrors.ustc.edu.cn/simple/</a></li>
<li>华中理工大学：<a href="http://pypi.hustunique.com/" target="_blank" rel="noopener">http://pypi.hustunique.com/</a></li>
<li>山东理工大学：<a href="http://pypi.sdutlinux.org/" target="_blank" rel="noopener">http://pypi.sdutlinux.org/</a> </li>
<li>豆瓣：<a href="http://pypi.douban.com/simple/" target="_blank" rel="noopener">http://pypi.douban.com/simple/</a></li>
</ul>
<p>使用：<br>假设使用清华镜像下载<code>tensorflow</code></p>
<pre class=" language-shell"><code class="language-shell">pip install tensorflow -i https://pypi.tuna.tsinghua.edu.cn/simple pyspider</code></pre>
<h3 id="1-2-设置配置文件，永久提速"><a href="#1-2-设置配置文件，永久提速" class="headerlink" title="1.2 设置配置文件，永久提速"></a>1.2 设置配置文件，永久提速</h3><p>修改 <code>~/.pip/pip.conf</code>(没有就创建一个文件夹及文件。文件夹要加“.”，表示是隐藏文件夹)<br>内容如下：</p>
<pre class=" language-py"><code class="language-py">[global]
index-url = https://pypi.tuna.tsinghua.edu.cn/simple</code></pre>
<p>亲测速度大幅度提升</p>
<h2 id="2-conda-提速"><a href="#2-conda-提速" class="headerlink" title="2 conda 提速"></a>2 conda 提速</h2><p>终端直接运行一下命令</p>
<pre class=" language-shell"><code class="language-shell"># 添加 Anaconda 的 TUNA 镜像
conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/
# TUNA 的 help 中镜像地址加有引号，需要去掉

# 设置搜索时显示通道地址
conda config --set show_channel_urls yes</code></pre>
<h2 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h2><ol>
<li><a href="http://www.cnblogs.com/dahu-daqing/p/6911414.html" target="_blank" rel="noopener">pip/conda国内镜像–安装包提速</a></li>
</ol>
]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>Pip</tag>
        <tag>Conda</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习性能评价指标汇总</title>
    <url>/2019/10/30/SummaryofMachineLearningPerformanceEvaluationIndicators/</url>
    <content><![CDATA[<h1 id="1-分类"><a href="#1-分类" class="headerlink" title="1 分类"></a>1 分类</h1><h2 id="1-1-混淆矩阵"><a href="#1-1-混淆矩阵" class="headerlink" title="1.1 混淆矩阵"></a>1.1 混淆矩阵</h2><ul>
<li>True Positive(真正, TP)：将正类预测为正类数.</li>
<li>True Negative(真负 , TN)：将负类预测为负类数.</li>
<li>False Positive(假正, FP)：将负类预测为正类数 → 误报 (Type I error).</li>
<li>False Negative(假负 , FN)：将正类预测为负类数 → 漏报 (Type II error).</li>
</ul>
<p>关系如下表所示：</p>
<table>
<thead>
<tr>
<th></th>
<th>预测值=1</th>
<th>预测值=0</th>
</tr>
</thead>
<tbody><tr>
<td>真实值=1</td>
<td>TP</td>
<td>FN</td>
</tr>
<tr>
<td>真实值=0</td>
<td>FP</td>
<td>TN</td>
</tr>
</tbody></table>
<h3 id="1-1-1-准确率-Accuracy-ACC"><a href="#1-1-1-准确率-Accuracy-ACC" class="headerlink" title="1.1.1 准确率 Accuracy, ACC"></a>1.1.1 准确率 Accuracy, ACC</h3><p>$$<br>ACC(Accuracy) = \frac{TP+TN}{TP+TN+FP+FN}<br>$$</p>
<blockquote>
<p>注：在正负样本不平衡的情况下，准确率这个评价指标有很大的缺陷。比如在互联网广告里面，点击的数量是很少的，一般只有千分之几，如果用acc，即使全部预测成负类（不点击）acc 也有 99% 以上，没有意义</p>
</blockquote>
<h3 id="1-1-2-精确率-或-查准率-Precision-P"><a href="#1-1-2-精确率-或-查准率-Precision-P" class="headerlink" title="1.1.2 精确率 或 查准率 Precision, P"></a>1.1.2 精确率 或 查准率 Precision, P</h3><p>$$<br>P=\frac{TP}{TP+FP}<br>$$</p>
<blockquote>
<p>注： 精确率(precision)和准确率(accuracy)是不一样的</p>
</blockquote>
<h3 id="1-2-3-召回率-或-查全率-Recall-R"><a href="#1-2-3-召回率-或-查全率-Recall-R" class="headerlink" title="1.2.3 召回率 或 查全率 Recall, R"></a>1.2.3 召回率 或 查全率 Recall, R</h3><p>$$<br>R=\frac{TP}{TP+FN}<br>$$</p>
<h3 id="1-2-4-F1-测量值"><a href="#1-2-4-F1-测量值" class="headerlink" title="1.2.4 F1 测量值"></a>1.2.4 F1 测量值</h3><p>$$<br>\frac{2}{F1} = \frac{1}{P} + \frac{1}{R} \<br>F1 = \frac{2TP}{2TP + FP + FN}<br>$$</p>
<blockquote>
<p>注： F1 是精确率和召回率的调和均值</p>
</blockquote>
<h2 id="1-3-AUC"><a href="#1-3-AUC" class="headerlink" title="1.3 AUC"></a>1.3 AUC</h2><p><code>AUC</code> 是 ROC (Receiver Operating Characteristic) 曲线以下的面积, 介于0.1和1之间。Auc作为数值可以直观的评价分类器的好坏，值越大越好。</p>
<h3 id="1-3-1-ROC-曲线"><a href="#1-3-1-ROC-曲线" class="headerlink" title="1.3.1 ROC 曲线"></a>1.3.1 ROC 曲线</h3><p>这里不赘述ROC的一些细节，参考<a href="https://cloud.tencent.com/developer/article/1107702" target="_blank" rel="noopener">ROC和AUC介绍以及如何计算AUC</a></p>
<p>ROC曲线关注两个指标：<br>$$<br>true<del>positive</del>rate:  TPR = \frac{TP}{TP + FN}\<br>false<del>positive</del>rate: FPN = \frac{FP}{FP + TN}<br>$$</p>
<p>ROC  曲线如图(a)所示，横坐标是<code>false positive rate, FPN</code>, 纵坐标是<code>true positive rate, TPR</code></p>
<p><img src="introduction-to-auc-and-roc.png" alt="在这里插入图片描述"></p>
<ul>
<li>横轴FPR:$1-TNR$, $1-Specificity$，$FPR$越大，预测正类中实际负类越多。</li>
<li>纵轴TPR：$Sensitivity$(正类覆盖率), $TPR$越大，预测正类中实际正类越多。</li>
<li>理想目标：$TPR=1$，$FPR=0$, 即图中(0,1)点，故ROC曲线越靠拢(0,1)点，越偏离45度对角线越好，$Sensitivity$、$Specificity$越大效果越好。</li>
</ul>
<p><strong>首先AUC值是一个概率值，当你随机挑选一个正样本以及负样本，当前的分类算法根据计算得到的Score值将这个正样本排在负样本前面的概率就是AUC值，AUC值越大，当前分类算法越有可能将正样本排在负样本前面，从而能够更好地分类。</strong></p>
<p>简单说：AUC值越大的分类器，正确率越高:</p>
<ul>
<li>AUC=1，完美分类器，采用这个预测模型时，不管设定什么阈值都能得出完美预测。绝大多数预测的场合，不存在完美分类器。</li>
<li>0.5&lt;AUC&lt;1，优于随机猜测。这个分类器（模型）妥善设定阈值的话，能有预测价值。</li>
<li>AUC=0.5，跟随机猜测一样（例：丢铜板），模型没有预测价值。</li>
<li>AUC&lt;0.5，比随机猜测还差；但只要总是反预测而行，就优于随机猜测，因此不存在 AUC&lt;0.5 的情况。</li>
</ul>
<h3 id="1-3-2-为什么要使用ROC曲线-和-AUC-评价分类器"><a href="#1-3-2-为什么要使用ROC曲线-和-AUC-评价分类器" class="headerlink" title="1.3.2 为什么要使用ROC曲线 和 AUC 评价分类器"></a>1.3.2 为什么要使用ROC曲线 和 AUC 评价分类器</h3><p>既然已经这么多标准，为什么还要使用ROC和AUC呢？因为ROC曲线有个很好的特性：当测试集中的正负样本的分布变换的时候，ROC曲线能够保持不变。在实际的数据集中经常会出现样本类不平衡，即正负样本比例差距较大，而且测试数据中的正负样本也可能随着时间变化。下图是ROC曲线和Presision-Recall曲线的对比：<br><img src="presision-recallVSroc.png" alt="ROC曲线和Presision-Recall曲线的对比"></p>
<ul>
<li>(a)和 (c)为Roc曲线，(b)和(d)为Precision-Recall曲线。</li>
<li>(a)和(b)展示的是分类其在原始测试集(正负样本分布平衡)的结果，(c)(d)是将测试集中负样本的数量增加到原来的10倍后，分类器的结果，可以明显的看出，ROC曲线基本保持原貌，而Precision-Recall曲线变化较大。</li>
</ul>
<h1 id="2-回归"><a href="#2-回归" class="headerlink" title="2 回归"></a>2 回归</h1><h2 id="2-1-平均绝对误差"><a href="#2-1-平均绝对误差" class="headerlink" title="2.1  平均绝对误差"></a>2.1  平均绝对误差</h2><p>平均绝对误差$MAE$ (Mean Absolute Error) 又被称为 $l1$ <strong>范数损失</strong>($l1-norm~loss$)：<br>$$<br>MAE(y,\hat{y}) = \frac{1}{n} \sum_{i=1}^{n}|y_i−\hat{y_i}|<br>$$</p>
<h2 id="2-2-平均平方误差"><a href="#2-2-平均平方误差" class="headerlink" title="2.2 平均平方误差"></a>2.2 平均平方误差</h2><p>平均平方误差 $MSE$ (Mean Squared Error) 又被称为 $l2$ <strong>范数损失</strong>($l2-norm~loss$)：<br>$$<br>MSE(y,\hat{y}) = \frac{1}{n} \sum_{i=1}^{n}|y_i−\hat{y_i}|^2<br>$$</p>
<h2 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h2><ol>
<li><a href="http://www.cnblogs.com/dlml/p/4403482.htm" target="_blank" rel="noopener">分类器性能指标之ROC曲线、AUC值</a></li>
<li><a href="https://beader.me/2013/12/15/auc-roc/" target="_blank" rel="noopener">AUC与ROC - 衡量分类器的好坏</a></li>
</ol>
]]></content>
      <categories>
        <category>人工智能</category>
      </categories>
      <tags>
        <tag>最优化算法</tag>
      </tags>
  </entry>
  <entry>
    <title>牛顿法理解</title>
    <url>/2019/10/08/newtonsMethod/</url>
    <content><![CDATA[<h2 id="1-牛顿法"><a href="#1-牛顿法" class="headerlink" title="1. 牛顿法"></a>1. 牛顿法</h2><p><code>牛顿法(Newton&#39;s method)</code>是一种近似求解方程的方法，它使用函数$f(x)$的<code>泰勒级数</code>的前面几项来寻找方程$f(x)=0$的根，后来由于最优化研究兴起后，将其应用在了最优化领域。</p>
<p><code>牛顿法</code>和<code>梯度下降法</code>的目标一样，都是通过迭代法寻找导数为0的点，核心思想是在迭代点用二次函数（泰勒级数）来逼近目标函数，得到导数为0的方程，求解该方程，得到下一个迭代点，就这样不断迭代，直到到达导数为0的点处。</p>
<h2 id="2-牛顿法的代数方法理解"><a href="#2-牛顿法的代数方法理解" class="headerlink" title="2. 牛顿法的代数方法理解"></a>2. 牛顿法的代数方法理解</h2><p>我们同样先用易懂的<code>代数方法</code>进行描述，然后再理解<code>矩阵方法</code>的描述</p>
<p>如有有同学不太了解<code>泰勒级数</code>，可以先看<a href="https://zh.wikipedia.org/wiki/%E6%B3%B0%E5%8B%92%E7%BA%A7%E6%95%B0" target="_blank" rel="noopener">维基百科-泰勒级数</a>对其有个大致的了解就可以了。</p>
<p>假设目标函数$f(x)$, 我们对其在$x_0$处做泰勒展开，其公式如下：</p>
<p>$$<br>f(x) = f(x_0) + f’(x_0)(x-x_0) + \frac{1}{2} f’’(x_0)(x-x_0)^2 + \ldots + \frac{1}{n!}f^{(n)}(x_0)(x-x_0)^n<br>$$</p>
<p>我们忽略那些高阶无穷小的项，在这里也就是二次以上的项，也就是</p>
<p>$$<br>f(x) = f(x_0) + f’(x_0)(x-x_0) + \frac{1}{2} f’’(x_0)(x-x_0)^2<br>$$</p>
<p>现在再对等式两边其进行求导，并令其为0，即</p>
<p>$$<br>\begin{split}<br>f’(x) &amp;= 0 + f’(x_0) + \frac{1}{2} f’’(x_0) * (2(x-x_0)) \\\<br>&amp;= f’(x_0) + f’’(x_0)(x-x_0)<br>\end{split}<br>$$</p>
<p>令左式为0，即得</p>
<p>$$<br>0 = f’(x_0) + f’’(x_0)(x-x_0)<br>$$</p>
<p>可得</p>
<p>$$<br>x = x_0 - \frac{f’(x_0)}{f’’(x_0)}<br>$$</p>
<p>这样我们就得倒了下一个迭代点，只要重复这个过程直到是导数为0的点，这样我们就有了代数方法的牛顿迭代公式，如下</p>
<p>$$<br>x_{i+1} \leftarrow x_i - \frac{f’(x_i)}{f’’(x_i)}<br>$$</p>
<h3 id="2-2-牛顿法的矩阵方法理解"><a href="#2-2-牛顿法的矩阵方法理解" class="headerlink" title="2.2 牛顿法的矩阵方法理解"></a>2.2 牛顿法的矩阵方法理解</h3><p>在实际应用中我们主要面对向量和矩阵的情况。便于理解，在矩阵方法中会需要<code>矩阵微分</code>的基础知识，网上有很多相关资料，推荐<a href="https://en.wikipedia.org/wiki/Matrix_calculus" target="_blank" rel="noopener">wikipedia-Matrix_calculus</a></p>
<h4 id="2-2-1-泰勒展开"><a href="#2-2-1-泰勒展开" class="headerlink" title="2.2.1 泰勒展开"></a>2.2.1 泰勒展开</h4><p>首先我们先看二元函数在点$x^0即(x^0_1, x^0_2)$处的泰勒展开(只展开到二级)：</p>
<p>$$<br>\begin{split}<br>f(x_1, x_2) = &amp;f(x^0_1, x^0_2) \\\<br>&amp;+(x_1-x^0_1)f’_{x_1}(x^0_1, x^0_2) + (x_2-x^0_2)f’_{x_2}(x^0_1, x^0_2) \\\<br>&amp;+\frac{1}{2!}(x_1-x^0_1)^2f’’_{x_1x_1}(x^0_1, x^0_2) + \frac{1}{2!}(x_1-x^0_1)(x_2-x^0_2)f’’_{x_1x_2}(x^0_1, x^0_2) \\\<br>&amp;+\frac{1}{2!}(x_2-x^0_2)(x_1-x^0_1)f’’_{x_2x_1}(x^0_1, x^0_2) + \frac{1}{2!}(x_2-x^0_2)^2f’’_{x_2x_2}(x^0_1, x^0_2)\\\<br>&amp;+o^n<br>\end{split}<br>$$</p>
<p>下面对N元函数在$x^0即(x^0_1, x^0_2, \ldots, x^0_n)$处进行展开，也只展开到二级，如下：</p>
<p>$$<br>\begin{split}<br>f(x_1, x_2, \ldots, x_n) = &amp;f(x^0_1, x^0_2, \ldots, x^0_n) \\\<br>&amp; + \sum_{i=1}^{n} (x_i - x^0_i)f’_{x_i}(x^0_1, x^0_2, \ldots, x^0_n) \\\<br>&amp; + \frac{1}{2!}\sum_{i,j=1}^{n}(x_i-x^0_i)(x_j-x^0_j)f’’_{x_ix_j}(x^0_1, x^0_2, \ldots, x^0_n) \\\<br>&amp; + o^n<br>\end{split}<br>$$</p>
<p>好了，现在我们将其写成矩阵形式，在$\mathbf{x}^0$处展开，其中$\mathbf{x}^0=[x^0_1, x^0_2, \ldots, x^0_n]^T$，即为：</p>
<p>$$<br>\begin{split}<br>f(\mathbf{x})=f(\mathbf{x}^0) + [\nabla f(\mathbf{x}^0)]^T(\mathbf{x}-\mathbf{x}^0) + \frac{1}{2!}[\mathbf{x}-\mathbf{x}^0]^T\nabla^2f(\mathbf{x}^0)(\mathbf{x}-\mathbf{x}^0) + o^n<br>\end{split}<br>$$</p>
<p>其中：</p>
<p>$$<br>\nabla^2f(\mathbf{x}^0) = <br>\begin{bmatrix}<br>\frac{\partial^2f(\mathbf{x}^0)}{(\partial x_1)^2} &amp; \frac{\partial^2f(\mathbf{x}^0)}{\partial x_1 \partial x_2} &amp; \cdots &amp; \frac{\partial^2f(\mathbf{x}^0)}{\partial x_1 \partial x_n} \\\<br>\frac{\partial^2f(\mathbf{x}^0)}{\partial x_2 \partial x_1} &amp; \frac{\partial^2f(\mathbf{x}^0)}{(\partial x_2)^2} &amp; \cdots &amp; \frac{\partial^2f(\mathbf{x}^0)}{\partial x_2 \partial x_n} \\\<br>\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\\<br>\frac{\partial^2f(\mathbf{x}^0)}{\partial x_n \partial x_1} &amp; \frac{\partial^2f(\mathbf{x}^0)}{\partial x_n \partial x_1} &amp; \cdots &amp; \frac{\partial^2f(\mathbf{x}^0)}{(\partial x_n)^2} \\\<br>\end{bmatrix}<br>$$</p>
<p>这种矩阵也称之为<code>Hessian 矩阵</code>,<a href="https://en.wikipedia.org/wiki/Hessian_matrix" target="_blank" rel="noopener">wikipedia-Hessian_matrix</a></p>
<p>如果对上面还有不太理解的，可以先用二元自己尝试转换成矩阵形式来类推N元。</p>
<h4 id="2-2-2-迭代求解"><a href="#2-2-2-迭代求解" class="headerlink" title="2.2.2 迭代求解"></a>2.2.2 迭代求解</h4><p>我们对矩阵形式的泰勒展开忽略二次以上的项，即为：</p>
<p>$$<br>\begin{split}<br>f(\mathbf{x})=f(\mathbf{x}^0) + [\nabla f(\mathbf{x}^0)]^T(\mathbf{x}-\mathbf{x}^0) + \frac{1}{2!}[\mathbf{x}-\mathbf{x}^0]^T\nabla^2f(\mathbf{x}^0)(\mathbf{x}-\mathbf{x}^0)<br>\end{split}<br>$$</p>
<p>对其两边同时求梯度，我们可以得到：</p>
<p>$$<br>\nabla f(\mathbf{x})=  \nabla f(\mathbf{x}^0) + \nabla^2f(\mathbf{x}^0)(\mathbf{x}-\mathbf{x}^0)<br>$$</p>
<p>其中 $\nabla^2f(\mathbf{x}^0)$ 为<code>Hessian 矩阵</code>, 我们将其写为$\mathbf{H}$, 令函数的梯度为0，则有：</p>
<p>$$<br>\begin{split}<br>0 &amp;= \nabla f(\mathbf{x}^0) + \nabla^2f(\mathbf{x}^0)(\mathbf{x}-\mathbf{x}^0) \\\<br>\Rightarrow  \mathbf{x} &amp;= \mathbf{x}^0 - [\nabla^2f(\mathbf{x}^0)]^{-1} \nabla f(\mathbf{x}^0) \\\<br>&amp;= \mathbf{x}^0 - \mathbf{H}^{-1} \nabla f(\mathbf{x}^0)<br>\end{split}<br>$$</p>
<p>从$\mathbf{x}^0$处开始，反复计算函数在处的Hessian矩阵和梯度向量，然后用下述公式进行迭代：</p>
<p>$$<br>\mathbf{x}^{i+1} \leftarrow \mathbf{x}^i - \mathbf{H}_i^{-1} \nabla f(\mathbf{x}^i)<br>$$</p>
<p>最终会达到最优的点，其中$- \mathbf{H}_i^{-1} \nabla f(\mathbf{x}^i)$ 称之为牛顿方向，迭代终止的条件是梯度的模接近于0，或者函数值下降小于指定阈值。</p>
<h2 id="3-优缺点"><a href="#3-优缺点" class="headerlink" title="3. 优缺点"></a>3. 优缺点</h2><table>
<thead>
<tr>
<th>主题</th>
<th>内容</th>
</tr>
</thead>
<tbody><tr>
<td>优点</td>
<td>1. 充分接近极小点时，牛顿法具有二阶收敛速度</td>
</tr>
<tr>
<td>缺点</td>
<td>1. 和梯度下降法相比，在使用牛顿迭代法进行优化的时候，需要求Hessien矩阵的逆矩阵，这个开销是很大的。<br><br> 2. 牛顿法不是整体收敛的。</td>
</tr>
</tbody></table>
<h2 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h2><ol>
<li><a href="https://zh.wikipedia.org/wiki/%E6%B3%B0%E5%8B%92%E7%BA%A7%E6%95%B0" target="_blank" rel="noopener">维基百科-泰勒级数</a></li>
<li><a href="https://en.wikipedia.org/wiki/Matrix_calculus" target="_blank" rel="noopener">wikipedia-Matrix_calculus</a></li>
<li><a href="https://en.wikipedia.org/wiki/Hessian_matrix" target="_blank" rel="noopener">wikipedia-Hessian_matrix</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/37588590" target="_blank" rel="noopener">理解牛顿法</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/33316479" target="_blank" rel="noopener">多元函数的泰勒展开式</a></li>
<li><a href="https://www.codelast.com/%E5%8E%9F%E5%88%9B-%E5%86%8D%E8%B0%88-%E7%89%9B%E9%A1%BF%E6%B3%95newtons-method-in-optimization/" target="_blank" rel="noopener">再谈 牛顿法/Newton’s Method In Optimization</a></li>
</ol>
]]></content>
      <categories>
        <category>人工智能</category>
      </categories>
      <tags>
        <tag>最优化算法</tag>
      </tags>
  </entry>
  <entry>
    <title>最优化算法之Momentum</title>
    <url>/2019/10/08/GD-OptimizationAlgorithms-Momentum/</url>
    <content><![CDATA[<h2 id="1-Momentum"><a href="#1-Momentum" class="headerlink" title="1 Momentum"></a>1 Momentum</h2><blockquote>
<p>Momentum 中文为：动量法</p>
</blockquote>
<p>虽然随机梯度下降仍然是非常受欢迎的优化方法，但其学习过程有时会很慢，并且有的时候对于比较的复杂的模型，我们很难得到<code>global minima</code>， 当然他们的梯度在这些地方也是0，所以就再也不会更新参数值，一直卡在这些伪最优解上，如下图所示：</p>
<p><img src="GD-momentum-problems.jpg" alt="三种伪最优解"></p>
<p>这三种伪最优解为：</p>
<ol>
<li><strong>plateau</strong>: 稳定的水平</li>
<li><strong>saddle point</strong>: 鞍点</li>
<li><strong>local minima</strong>: 局部最小点</li>
</ol>
<p>为了解决这个问题，Polyak 想到使用物理学中的惯性来解决，如下图所示：</p>
<p><img src="GD-momentum-physical.jpg" alt="实际情况"></p>
<p>上面是物理中的一种现象，我们可以想象，把一个小球放在这种实际的轨道上，这个小球并不会在上面我们说的梯度为0的地方就停止下来。因为在实际中的事物都是有惯性的，也就是momentum。那我们如果想要解决上面的问题，很自然的就会想能不能也给我们的gradient descent加上一个冲量呢？让他在这些梯度等于0的地方也能够像现实生活中的那样，去冲出这些伪最优解。</p>
<p>下面给出<code>动量法</code>的参数更新公式：</p>
<p>$$<br>\begin{split}<br>v_t &amp; \leftarrow \beta v_{t-1} + \alpha \nabla_{\theta} J(\theta) \\\<br>\theta_t &amp; \leftarrow \theta_{t-1} -v_t<br>\end{split}<br>$$</p>
<p>其中，动量超参数 $\beta$ 满足 $0 \le \beta &lt; 1$ 。当 $\beta = 0$ 时，动量法等价于小批量随机梯度下降。</p>
<p>这是比较直观的理解<code>动量法</code>，网上还有从数学角度也就是<code>指数移动平均</code>来理解 <code>动量法</code>，下文中将对其进行讲解</p>
<h2 id="2-指数加权移动平均"><a href="#2-指数加权移动平均" class="headerlink" title="2 指数加权移动平均"></a>2 指数加权移动平均</h2><p>在介绍<code>指数加权移动平均</code>之前先介绍一下<code>简单的移动平均法</code>以及<code>加权移动平均法</code></p>
<h3 id="2-1-简单移动平均法"><a href="#2-1-简单移动平均法" class="headerlink" title="2.1 简单移动平均法"></a>2.1 简单移动平均法</h3><p><code>简单移动平均法</code> 顾名思义，就是$t$前$N$个状态下值的平均值作为第$t$状态下的值，其公式如下：</p>
<p>$$<br>F_{t} = \frac{(x_t+x_{t-1}+ x_{t-2}+\ldots + x_{t-(N-1)})}{N} = \frac{1}{N} \sum_{i = t-N+1}^{t} x_i<br>$$<br>公式解释如下：</p>
<ul>
<li>$F_{t}$: 第$t$个状态下的预测值</li>
<li>$N$: 需要计算的移动状态个数</li>
<li>$x_i, (i=t, t-1, \ldots , t-N+1)$: 第$i$状态下的值</li>
</ul>
<p>其优缺点为：</p>
<ol>
<li><p><strong>优点</strong>: </p>
<ul>
<li>计算量少</li>
<li>移动平均线能较好的反应时间序列的趋势以及变化</li>
</ul>
</li>
<li><p><strong>缺点</strong>:</p>
<ul>
<li>计算移动平均必须具有$N$个过去观察值，当需要预测大量的数值时，就必须存储大量数据</li>
<li>$N$个过去观察值中每一个权数都相等，而早于$(t-N+1)$期的观察值的权数等于0，而实际上往往是最新观察值包含更多信息，因具有更大的权重。</li>
</ul>
</li>
</ol>
<h3 id="2-2-加权移动平均法"><a href="#2-2-加权移动平均法" class="headerlink" title="2.2 加权移动平均法"></a>2.2 加权移动平均法</h3><p>为了解决<code>简单移动平均法</code> 中的第二个缺点中的权重问题，就有了<code>加权移动平均法</code>， 其公式为：</p>
<p>$$<br>F_{t} = \frac{(w_t x_t+x_{t-1}+ w_{t-2} x_{t-2}+\ldots + w_{t-(N-1)} x_{t-(N-1)})}{N} = \frac{1}{N} \sum_{i = t-N+1}^{t} w_i x_i<br>$$</p>
<p>其中$w_i$ 是第 $i$ 状态下的权值。</p>
<p>在运用加权平均时，权重的选择是一个应该注意的问题，经验法和试算法使选择权重最简单的方法。一般而言，最近期的数据最能预测未来的情况。因而权重应大一些。</p>
<p>其优缺点为：</p>
<ol>
<li><p><strong>优点</strong>:</p>
<ul>
<li>使用加权移动平均法能平滑掉突然波动对预测结果的影响</li>
</ul>
</li>
<li><p><strong>缺点</strong>:</p>
<ul>
<li>加大移动平均法的期数会使平滑波动效果更好，但会使预测值对数据实际变动更不敏感, 也就是图像会往右移动，有时延</li>
<li>移动平均值并不能总是很好的反应出趋势。由于是平均值，预测值总是停留在过去的水平上而无法预计会导致将来更高或更低的波动。</li>
<li>移动平均法要大量的过去数据记录</li>
<li>需要不断修改平均值，以之作为预测值。</li>
</ul>
</li>
</ol>
<h3 id="2-3-指数加权移动平均法"><a href="#2-3-指数加权移动平均法" class="headerlink" title="2.3 指数加权移动平均法"></a>2.3 指数加权移动平均法</h3><p>指数平滑法是对加权移动平均法的改进，它是将前期预测值和前期实际值分别确定不同的权数(二者权数和为1)。只需要三个数据，所有预测方法中，指数平滑法采用较多，常用语短期预测，指数平滑法有很多种，有一次指数平滑预测、二次指数平滑预测以及三次指数平滑预测。在这里这说一次指数平滑预测。</p>
<p>给定超参数 $0 \le \beta &lt; 1$，利用前一期的预测值 $F_{t-1}$ 以及当前时间步另一变量 $x_t$ 的线性组合：</p>
<p>$$<br>F_t \leftarrow \beta F_{t-1} + (1-\beta)x_t<br>$$</p>
<p>选择合适的 $\beta$ 值。实际需求稳定，选取较小的 $\beta$ 值，反之选取较大的 $\beta$ 值。</p>
<p>为了便于理解，可以参考吴恩达老师的 slides， </p>
<blockquote>
<p>注：其中温度符号用错了</p>
</blockquote>
<p><img src="GD-exm-ag.jpg" alt></p>
<p>其实这里的曲线就是当 $v_0=0$作为移动平均的初始值，然后将对应的实际的温度值带入递归式子中，然后得出的曲线。其实从上面也可以看出来 $\beta$ 的选择尤为的重要。这个温度的例子$\beta=0.9$。可以看出曲线要平坦一点，这是因为你平均了几天的温度，所以这个曲线波动更小，更加平坦，缺点就是曲线会失去时效性，在图中的表现就是曲线会向右移动，那因为现在要平均的温度值更多，要平均更多的值，指数加权平均公式在温度变化的时，能更加适应缓慢一些，所以会出现一定的延迟。</p>
<p><img src="GD-exm-ag-2.jpg" alt></p>
<ul>
<li>$\beta = 0.9$的时候，曲线会更加平缓(稳定性高)，但是趋势曲线会向右移动(时效性差)</li>
<li>$\beta = 0.5$的时候，由于只平均了两天的温度，平均的数据太少了，得到的曲线会有更多的噪声也就是(稳定性差)更有可能出现异常值，但是趋势曲线能够更加适应拟合你的原始数据，也就是趋势曲线的(时效性高)</li>
</ul>
<p>可以看到$\beta$的值直接决定了平均的天数，下面将使用公式进行推导</p>
<p>我们可以对其进行展开：</p>
<p>$$<br>\begin{split}<br>F_t &amp; = \beta F_{t-1} + (1-\beta) x_t \\\<br>&amp;= \beta^2 F_{t-2} + (1-\beta)\beta x_{t-1} + (1-\beta) x_t \\\<br>&amp;= \beta^3 F_{t-3} + (1-\beta)\beta^2 x_{t-2} + (1-\beta)\beta x_{t-1} + (1-\beta) x_t \\\<br>&amp;= \ldots<br>\end{split}<br>$$</p>
<p>令$n = \frac{1}{1-\beta}$, 那么$(1-\frac{1}{n})^n = \beta^{\frac{1}{1-\beta}}$</p>
<p>因为<br>$$<br>\lim_{n \to \infty }(1-\frac{1}{n})^n = \exp(-1)<br>$$</p>
<p>所以当$\beta \to 1$ 时，$\beta^{\frac{1}{1-\beta}} = \exp(-1)$, 如$0.95^20 \approx \exp(-1)$， 如果把 $\exp(−1)$ 当作一个比较小的数，我们可以在近似中忽略所有含 $\beta^{\frac{1}{1-\beta}}$ 和比 $\beta^{\frac{1}{1-\beta}}$ 更高阶的系数的项。例如，当 $\beta=0.95$ 时，</p>
<p>$$<br>\begin{split}<br>F_t &amp;= (1-\beta)(\beta^0 x_t + \beta^1 x_{t-1} + \ldots + \beta^19x_{t-19} + \ldots) \\\<br>&amp;= 0.05\sum_{i=0}^{19} 0.95^i x_{t-i}<br>\end{split}<br>$$</p>
<p>因此，在实际中，我们常常将 $F_t$ 看作是对最近 $\frac{1}{1-\beta}$个时间步的 $x_t$ 值的加权平均。例如，当 $\beta=0.95$ 时， $F_t$ 可以被看作对最近$20$个时间步值的加权平均；当 $\beta=0.9$ 时， $F_t$ 可以看作是对最近$10$个时间步值的加权平均。而且，离当前时间步 $t$ 越近的 $x_t$ 值获得的权重越大（越接近1）。</p>
<h2 id="3-由指数加权移动平均理解动量法"><a href="#3-由指数加权移动平均理解动量法" class="headerlink" title="3 由指数加权移动平均理解动量法"></a>3 由指数加权移动平均理解动量法</h2><p>现在，我们对动量法的速度变量做变形：</p>
<p>$$<br>v_t \leftarrow \beta v_{t-1} + (1-\beta)(\frac{1}{1-\beta} \alpha \nabla_{\theta}(\theta))<br>$$</p>
<p>由指数加权移动平均的形式可得，速度变量 $v_t$ 实际上对序列<br>$$<br>\left \{ \frac{\alpha \nabla^{t-i}_{\theta}(\theta)}{1-\beta}, i=0,1,\ldots, \frac{1}{1-\beta} -1 \right \}<br>$$</p>
<p>做了指数加权移动平均。换句话说，相比于小批量随机梯度下降，动量法在每个时间步的自变量更新量近似于将前者对应的最近$\frac{1}{1-\beta}$ 个时间步的更新量做了指数加权移动平均后再除以$1-\beta$, 所以，在动量法中，自变量在各个方向上的移动幅度不仅取决当前梯度，还取决于过去的各个梯度在各个方向上是否一致。</p>
<h2 id="4-算法过程"><a href="#4-算法过程" class="headerlink" title="4 算法过程"></a>4 算法过程</h2><p><img src="momentum-algorithm.png" alt="使用动量的随机梯度下降伪代码"></p>
<h2 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h2><ol>
<li><a href="https://raw.githubusercontent.com/JDwangmo/deepLearningBook/master/book/deeplearning-%E5%B8%A6%E5%8F%82%E8%80%83%E6%96%87%E7%8C%AE.pdf" target="_blank" rel="noopener">Deep learning - Ian Goodfellow, Yoshua Bengio, Aaron Courville</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/34240246" target="_blank" rel="noopener">优化算法之Gradient descent with momentum</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/32335746" target="_blank" rel="noopener">优化算法之指数移动加权平均</a></li>
</ol>
]]></content>
      <categories>
        <category>人工智能</category>
      </categories>
      <tags>
        <tag>最优化算法</tag>
      </tags>
  </entry>
  <entry>
    <title>梯度下降概述</title>
    <url>/2019/09/26/summaryOfGradientDescent/</url>
    <content><![CDATA[<h2 id="0-前言"><a href="#0-前言" class="headerlink" title="0 前言"></a>0 前言</h2><p>在求解机器学习算法的模型参数，即无约束优化问题时，梯度下降算法是机器学习中使用非常广泛的算法之一。几乎当前每一个机器学习库或者深度学习库都会包括梯度下降算法的不同变种实现。它们就像一个黑盒优化器，很难得到它们优缺点的实际解释。</p>
<p>本文将对<code>梯度</code>、<code>梯度下降</code>等基础知识点、<code>三大梯度下降法框架</code>、<code>梯度下降优化算法</code>以及目前工业上使用的<code>并行分布式优化算法</code>进行讲解。</p>
<p>其中<code>梯度下降法算法详解</code> 参考自<a href="https://www.cnblogs.com/pinard/p/5970503.html" target="_blank" rel="noopener">刘建平Pinard - 梯度下降（Gradient Descent）小结</a>，刘老师通过代数方法引入通俗易懂，然后再讲解矩阵方法。<code>梯度下降框架</code>以及<code>梯度下降优化算法</code> 参考自<a href="http://ruder.io/optimizing-gradient-descent/index.html" target="_blank" rel="noopener">An overview of gradient descent optimization algorithms</a>, 这篇文章讲解非常全面，建议深入理解原文，<code>一只鸟的天空</code>对其进行了翻译，参考：<a href="https://blog.csdn.net/heyongluoyao8/article/details/52478715" target="_blank" rel="noopener">梯度下降优化算法综述</a></p>
<h2 id="1-基础知识点"><a href="#1-基础知识点" class="headerlink" title="1 基础知识点"></a>1 基础知识点</h2><h3 id="1-1-导数"><a href="#1-1-导数" class="headerlink" title="1.1 导数"></a>1.1 导数</h3><p>导数的几何意义可能很多人都比较熟悉: 当函数定义域和取值都在实数域中的时候，导数可以表示函数曲线上的切线斜率。 除了切线的斜率，导数还表示函数在该点的变化率。</p>
<p>$$<br>f’(x_0) = \lim_{\Delta x \to 0} \frac{\Delta y}{\Delta x} = \lim_{\Delta x \to 0} \frac {f(x_0 + \Delta x) - f(x_0)}{\Delta x}<br>$$</p>
<p><img src="./Derivative.png" alt="导数几何意义"></p>
<ul>
<li>$\Delta x$: $x$的变化量</li>
<li>$\mathrm{d}x$：$x$的变化量$\Delta x \to 0$时，则记作微元$dx$；</li>
<li>$\Delta y$：$\Delta y = f(x_0 + \Delta x) - f(x_0)$，是函数的增量；</li>
<li>$\mathrm{d}y$：$\mathrm{d}y=f’(x_0)\mathrm{d}x$，是切线的增量；</li>
<li>当$\Delta x \to 0$时，$\mathrm{d}y$与$\Delta y$都是无穷小，$\mathrm{d}y$是$\Delta y$的主部，即$\Delta y = \mathrm{d}y+ o(\Delta x)$.</li>
</ul>
<p>注意:反映的是函数$y=f(x)$在某一点处沿$x$轴正方向的变化率/变化趋势。直观地看，也就是在$x$轴上某一点处，如果$f’(x)&gt;0$，说明$f(x)$的函数值在$x$点沿$x$轴正方向是趋于增加的；如果$f’(x)&lt;0$，说明$f(x)$的函数值在$x$点沿$x$轴正方向是趋于减少的。</p>
<h3 id="1-2-偏导数"><a href="#1-2-偏导数" class="headerlink" title="1.2 偏导数"></a>1.2 偏导数</h3><p>偏导数定义如下：</p>
<p>$$<br>\frac{\partial f(x_0, x_1, \ldots, x_n)}{\partial x_j} = \lim_{\Delta x \to 0} \frac{\Delta y}{\Delta x} = \lim_{\Delta x \to 0} \frac {f(x_0, \ldots, x_j+ \Delta x, \ldots, x_n) - f(x_0, \ldots , x_j, \ldots, x_n)}{\Delta x}<br>$$<br>可以看到，导数与偏导数本质是一致的，都是当自变量的变化量趋于0时，函数值的变化量与自变量变化量比值的极限。直观地说，偏导数也就是函数在某一点上沿坐标轴<strong>正方向</strong>的的变化率。</p>
<blockquote>
<p>偏导数和导数的区别</p>
<blockquote>
<p>导数，指的是一元函数中，函数$y=f(x)$在某一点处沿x轴正方向的变化率；</p>
<p>偏导数，指的是多元函数中，函数$y=f(x_0,x_1,\ldots,x_n)$在某一点处沿某一坐标轴正方向的变化率。</p>
</blockquote>
</blockquote>
<h3 id="1-3-方向导数"><a href="#1-3-方向导数" class="headerlink" title="1.3 方向导数"></a>1.3 方向导数</h3><p>方向导数定义如下<br>$$<br>\frac{\partial f(x_0, x_1, \ldots, x_n)}{\partial l} = \lim_{\rho \to 0} \frac{\Delta y}{\Delta x} = \lim_{ \rho \to 0} \frac {f(x_0 + \Delta x_0, \ldots, x_j+ \Delta x, \ldots, x_n + \Delta x_n) - f(x_0, \ldots , x_j, \ldots, x_n)}{\rho}<br>$$<br>其中<br>$$<br>\rho = \sqrt{(\Delta x_0)^2 + (\Delta x_1)^2 + \ldots + (\Delta x_n)^2}<br>$$</p>
<p>在前面导数和偏导数的定义中，均是沿坐标轴正方向讨论函数的变化率。那么当我们讨论函数<strong>沿任意方向</strong>的变化率时，也就引出了方向导数的定义，即：某一点在某一<strong>趋近方向</strong>上的<strong>导数值</strong>。特就是说：我们不仅要知道函数在坐标轴正方向上的变化率（即偏导数），而且还要设法求得函数在其他特定方向上的变化率。而方向导数就是函数在其他特定方向上的变化率。</p>
<p>例题：<br>设二元函数$f(x, y) = x^2 + y^2$, 分别计算这个函数在点$(1,2)$沿方向$w = \{3,-4\}$与方向$u=\{1,0\}$ 的方向导数</p>
<p><strong>解</strong>：由于$w$不是单位向量，所以将其单位化：$v = w^0 = \frac{w}{|w|} = \{\frac {3}{5}, -\frac{4}{5}\}$</p>
<p>$$<br>\begin{split}<br>\therefore f(x_0 + tv_1, y_0 + tv_2) - f(x_0,y_0) &amp;= f(1 + \frac{3}{5}t, 2 - \frac{4}{5}t) - f(1,2) \\\<br>&amp;=[(1+\frac{3}{5}t)^2 + (2-\frac{4}{5})^2] - (1^2+2^2) \\\<br>&amp;=t^2-2t<br>\end{split}<br>$$</p>
<p>$$<br>\begin{split}<br>\therefore \left. \frac{\partial f}{\partial w} \right |_{(1,2)} &amp;= \lim_{t \to 0} \frac{t^2-2t}{t} = -2 \\\<br>\left. \frac{\partial f}{\partial u} \right |_{(1,2)} &amp;=\lim_{t \to 0} \frac{f(1+t, 2) - f(1,2)}{t} = \lim_{t \to 0} \frac{t^2+2t}{t} = 2<br>\end{split}<br>$$</p>
<h3 id="1-4-梯度"><a href="#1-4-梯度" class="headerlink" title="1.4 梯度"></a>1.4 梯度</h3><p>梯度的定义如下：</p>
<p>$$<br>\nabla f(x_0, x_1, \ldots, x_n) = \left( \frac{\partial f}{\partial x_0}, \frac{\partial f}{\partial x_1}, \ldots, \frac{\partial f}{\partial x_n}\right)<br>$$</p>
<blockquote>
<p>梯度符号可以用 $\nabla f$, 也可以用 $\operatorname{grad} f$ 表示</p>
</blockquote>
<p>那么这个梯度向量求出来有什么意义呢？他的意义从几何意义上讲，就是函数变化增加最快的地方。具体来说，对于函数$f(x,y)$,在点$(x0,y0)$，沿着梯度向量的方向就是$(\frac{\partial f}{\partial x_0}, \frac{\partial f}{\partial y_0})^T$的方向是$f(x,y)$增加最快的地方。或者说，沿着梯度向量的方向，更加容易找到函数的最大值。反过来说，沿着梯度向量相反的方向，也就是 $-(\frac{\partial f}{\partial x_0}, \frac{\partial f}{\partial y_0})^T$的方向，梯度减少最快，也就是更加容易找到函数的最小值。</p>
<p>简单来说梯度的提出只为回答一个问题：函数在变量空间的某一点处，沿着哪一个方向有最大的变化率？</p>
<p>这里注意三点:</p>
<ul>
<li>梯度是一个向量，即有方向有大小；</li>
<li>梯度的方向是最大方向导数的方向；</li>
<li>梯度的值是最大方向导数的值。</li>
</ul>
<h2 id="2-梯度下降"><a href="#2-梯度下降" class="headerlink" title="2 梯度下降"></a>2 梯度下降</h2><p>在找了一些现有材料，发现刘建平老师的<a href="https://www.cnblogs.com/pinard/p/5970503.html" target="_blank" rel="noopener">梯度下降（Gradient Descent）小结</a> 讲解的最为全面易懂，如果看完这小结有疑问的可以去网上找相关知识点进行学习消化。</p>
<h3 id="2-1-梯度下降直观解释"><a href="#2-1-梯度下降直观解释" class="headerlink" title="2.1 梯度下降直观解释"></a>2.1 梯度下降直观解释</h3><p>首先来看看梯度下降的一个直观的解释。比如我们在一座大山上的某处位置，由于我们不知道怎么下山，于是决定走一步算一步，也就是在每走到一个位置的时候，求解当前位置的梯度，沿着梯度的负方向，也就是当前最陡峭的位置向下走一步，然后继续求解当前位置梯度，向这一步所在位置沿着最陡峭最易下山的位置走一步。这样一步步的走下去，一直走到觉得我们已经到了山脚。当然这样走下去，有可能我们不能走到山脚，而是到了某一个局部的山峰低处。</p>
<p><img src="Gd_graph.png" alt="梯度下降直观解释"></p>
<p>从上面的解释可以看出，梯度下降不一定能够找到全局的最优解，有可能是一个局部最优解。当然，如果损失函数是凸函数，梯度下降法得到的解就一定是全局最优解。</p>
<h3 id="2-2-梯度下降的基本概念"><a href="#2-2-梯度下降的基本概念" class="headerlink" title="2.2 梯度下降的基本概念"></a>2.2 梯度下降的基本概念</h3><p>在详细了解梯度下降的算法之前，我们先看看相关的一些概念。</p>
<ol>
<li><p>步长（Learning rate）：步长决定了在梯度下降迭代的过程中，每一步沿梯度负方向前进的长度。用上面下山的例子，步长就是在当前这一步所在位置沿着最陡峭最易下山的位置走的那一步的长度。</p>
</li>
<li><p>特征（feature）：指的是样本中输入部分，比如2个单特征的样本$(x_0^0,y^0), (𝑥_0^1,y^1)$,则第一个样本特征为$x_0^0$，第一个样本输出为$y_0$。</p>
</li>
<li><p>假设函数（hypothesis function）：在监督学习中，为了拟合输入样本，而使用的假设函数，记为$h_\theta(x)$。比如对于单个特征的$m$个样本$(x_0^j,y^j)(j=1,2,\ldots ,m)$,可以采用拟合函数如下： $h_{\theta}(x) = \theta_0 + \theta_1 x$。</p>
</li>
<li><p>损失函数（loss function）：为了评估模型拟合的好坏，通常用损失函数来度量拟合的程度。损失函数极小化，意味着拟合程度最好，对应的模型参数即为最优参数。在线性回归中，损失函数通常为样本输出和假设函数的差取平方。比如对于$m$个样本$(x^j,y^j)(j=1,2,…m)$,采用线性回归，损失函数为：</p>
</li>
</ol>
<p>$$<br>J(\theta_0, \theta_1) = \sum_{j=1}^{m} (h_{\theta}(x^j) - y^j)^2<br>$$</p>
<p>其中$x^j$表示第$j$个样本特征，$y^j$表示第$j$个样本对应的输出，$h_{\theta}(x^j)$为假设函数。</p>
<h3 id="2-3-梯度下降的详细算法"><a href="#2-3-梯度下降的详细算法" class="headerlink" title="2.3 梯度下降的详细算法"></a>2.3 梯度下降的详细算法</h3><p>梯度下降法的算法可以有代数法和矩阵法（也称向量法）两种表示，如果对矩阵分析不熟悉，则代数法更加容易理解。不过矩阵法更加的简洁，且由于使用了矩阵，实现逻辑更加的一目了然。这里先介绍代数法，后介绍矩阵法。</p>
<p>推荐一个官方的<a href="https://developers.google.com/machine-learning/crash-course/fitter/graph?hl=zh-cn" target="_blank" rel="noopener">迭代可视化</a>，如下图所示，一边看一边操作可以快速理解。</p>
<p><img src="Gd_google.png" alt="谷歌官方机器学习课程-优化学习速率"></p>
<h4 id="2-3-1-梯度下降法的代数方式描述"><a href="#2-3-1-梯度下降法的代数方式描述" class="headerlink" title="2.3.1 梯度下降法的代数方式描述"></a>2.3.1 梯度下降法的代数方式描述</h4><ol>
<li><p><strong>先决条件</strong>: 确认优化模型的假设函数和损失函数。</p>
<p> 比如对于线性回归，假设函数表示为 $h_\theta(x_1^j, x_2^j, \ldots , x_n^j) = \theta_0 + \theta_1 x_1^j + \theta_2 x_2^j + \ldots + \theta_n x_n^j$, 其中$\theta_i, (i=1,2,\ldots ,n)$ 为模型参数，$x_i^j,(i=1,2,\ldots ,n)$ 为第$j$个样本的$n$个特征。这个表示可以简化，我们增加一个特征$x_0^j = 1$， 这样就可以简化成<br> $$<br> h_\theta(x_1^j, x_2^j, \ldots , x_n^j) = \sum_{i=0}^{n} \theta_l x_i^j<br> $$</p>
<p> 同样是线性回归，对应于上面的假设函数，损失函数为：<br> $$<br> J( \theta_0 , \theta_1, \ldots , \theta_n) = \frac{1}{2m} \sum_{j = 0}^{m}  (h_\theta(x_0^j, x_1^j, \ldots, x_n^j) - y^j)^2<br> $$</p>
</li>
</ol>
<ol start="2">
<li><p><strong>算法相关参数初始化</strong>: 主要是初始化$\theta_0,\theta_1, \ldots, \theta_n$,算法终止距离$\epsilon$以及步长$\alpha$。在没有任何先验知识的时候，可以将所有的$theta$初始化为0， 将步长初始化为1。在调优的时候再优化。</p>
</li>
<li><p><strong>算法过程</strong>:</p>
<ol>
<li>确定当前位置的损失函数的梯度，对于$\theta_i$,其梯度表达式: $\frac{\partial J( \theta_0 , \theta_1, \ldots , \theta_n)}{\partial \theta_i}$</li>
<li>用步长乘以损失函数的梯度，得到当前位置下降的距离，即$\alpha \frac{\partial J( \theta_0 , \theta_1, \ldots , \theta_n)}{\partial \theta_i}$对应于前面登山例子中的某一步。</li>
<li>确定是否所有的$\theta_i$,梯度下降的距离都小于$\epsilon$，如果小于$\epsilon$则算法终止，当前所有的$\theta_i, (i=0,1,…n)$即为最终结果。否则进入步骤4.</li>
<li>更新所有的$\theta$，对于$\theta_i$，其更新表达式如下。更新完毕后继续转入步骤1.<br> $$<br> \theta_i \leftarrow \theta_i - \alpha \frac{\partial J( \theta_0 , \theta_1, \ldots , \theta_n)}{\partial \theta_i}<br> $$</li>
</ol>
</li>
</ol>
<p>下面用线性回归的例子来具体描述梯度下降。假设我们的样本是<br>$$<br>\begin{split}<br>&amp; (x_1^0, x_2^0, \ldots, x_n^0, y^0), \\\<br>&amp; (x_1^1, x_2^1, \ldots, x_n^1, y^1), \\\<br>&amp; \cdots \\\<br>&amp; (x_1^m, x_2^m, \ldots, x_n^m, y^m)<br>\end{split}<br>$$</p>
<p>损失函数如前面先决条件所述：<br>$$<br>J( \theta_0 , \theta_1, \ldots , \theta_n) = \frac{1}{2m} \sum_{j = 0}^{m}  (h_\theta(x_0^j, x_1^j, \ldots, x_n^j) - y^j)^2<br>$$</p>
<p>则在算法过程步骤1中对于$\theta_i$ 的偏导数计算如下：<br>$$<br>\frac{\partial J( \theta_0 , \theta_1, \ldots , \theta_n)}{\partial \theta_i} = \frac{1}{m} \sum_{j = 0}^{m}  (h_\theta(x_0^j, x_1^j, \ldots, x_n^j) - y^j)x_i^j<br>$$</p>
<p>由于样本中没有$x_0^j$, 上式中令所有的$x_0^j = 1$为1.<br>步骤4中$\theta_i$的更新表达式如下：</p>
<p>$$<br>\theta_i \leftarrow \theta_i - \alpha \frac{1}{m} \sum_{j = 0}^{m}  (h_\theta(x_0^j, x_1^j, \ldots, x_n^j) - y^j)x_i^j<br>$$</p>
<p>从这个例子可以看出当前点的梯度方向是由所有的样本决定的，加  $\frac{1}{m}$ 是为了好理解。由于步长也为常数，他们的乘机也为常数，所以这里$  \alpha \frac{1}{m}$可以用一个常数表示。</p>
<p>在下面第2.4节会详细讲到的梯度下降法的变种，他们主要的区别就是对样本的采用方法不同。这里我们采用的是用<strong>所有样本</strong>。</p>
<h4 id="2-3-2-梯度下降法的矩阵方式描述"><a href="#2-3-2-梯度下降法的矩阵方式描述" class="headerlink" title="2.3.2 梯度下降法的矩阵方式描述"></a>2.3.2 梯度下降法的矩阵方式描述</h4><p>这一部分主要讲解梯度下降法的矩阵方式表述，相对于3.3.1的代数法，要求有一定的矩阵分析的基础知识，尤其是矩阵求导的知识。</p>
<p>传送门：<a href="https://en.wikipedia.org/wiki/Matrix_calculus" target="_blank" rel="noopener">矩阵微分</a>，这里面有用到所有的矩阵求导微分公式，如果需要深入理解矩阵求导建议参考张贤达的<strong>《矩阵分析与应用》</strong>一书。</p>
<ol>
<li><p><strong>先决条件</strong>:和2.3.1类似， 需要确认优化模型的假设函数和损失函数。对于线性回归，假设函数$h_\theta(x_1^j, x_2^j, \ldots , x_n^j) = \theta_0 + \theta_1 x_1^j + \theta_2 x_2^j + \ldots + \theta_n x_n^j$ 的矩阵表达式为：<br> $$<br> h_\theta(\mathbf{X}) = \mathbf{X}\theta<br> $$<br> 其中， 假设函数$h_\theta(X)$为$m \times     1$的向量,$\theta$为$(n+1) \times 1$的向量，里面有$n+1$个代数法的模型参数。$\mathbf{X}$为$m \times (n+1)$维的矩阵。$m$代表样本的个数，$n+1$代表样本的特征数。</p>
<p> 损失函数的表达式为：<br> $$<br> J(\theta) = \frac{1}{2} (\mathbf{X}\theta - Y)^T(\mathbf{X}\theta - Y)<br> $$</p>
<p> 其中$Y$是样本的输出向量，维度为$m \times    1$.</p>
</li>
<li><p><strong>算法相关参数初始化</strong>: $\theta$向量可以初始化为默认值，或者调优后的值。算法终止距离$\epsilon$，步长$\alpha$和2.3.1比没有变化。</p>
</li>
<li><p><strong>算法过程</strong>:</p>
<ol>
<li>确定当前位置的损失函数的梯度，对于$\theta$向量,其梯度表达式为：$\frac{\partial J(\theta)}{\partial \theta}$</li>
<li>用步长乘以损失函数的梯度，得到当前位置下降的距离，即$\alpha \frac{\partial J(\theta)}{\partial \theta}$对应于前面登山例子中的某一步.</li>
<li>确定$\theta$向量里面的每个值,梯度下降的距离都小于$\epsilon$，如果小于$\epsilon$则算法终止，当前$\theta$向量即为最终结果。否则进入步骤4. </li>
<li>更新$\theta$向量，其更新表达式如下。更新完毕后继续转入步骤1.<br> $$<br> \theta \leftarrow \theta - \alpha \frac{\partial J(\theta)}{\partial \theta}<br> $$</li>
</ol>
</li>
</ol>
<p>还是用线性回归的例子来描述具体的算法过程。<br>损失函数对于$\theta$向量的偏导数计算如下：<br>$$<br>\frac{\partial J(\theta)}{\partial \theta} = \mathbf{X}^T(\mathbf{X}\theta - Y)<br>$$</p>
<p>步骤4中$\theta$向量的更新表达式如下：</p>
<p>$$<br>\theta \leftarrow \theta - \alpha \mathbf{X}^T(\mathbf{X}\theta - Y)<br>$$</p>
<p>对于2.3.1的代数法，可以看到矩阵法要简洁很多。这里面用到了矩阵求导链式法则，和两个矩阵求导的公式。</p>
<p>这里面用到了矩阵求导链式法则，和两个个矩阵求导的公式。</p>
<p>公式1:<br>$$<br>\frac{\partial (\mathbf{x}^T\mathbf{x})}{\partial \mathbf{x}} = 2\mathbf{x}<br>$$<br>其中$\mathbf{x}$为向量。</p>
<p>公式2:<br>$$<br>\begin{split}<br>\nabla_{X} f(AX+B) &amp;= A^T \nabla_{Y} f \\\<br>Y &amp;= AX+B<br>\end{split}<br>$$<br>其中$f(Y)$ 为标量。</p>
<h3 id="2-4-梯度下降的算法调优"><a href="#2-4-梯度下降的算法调优" class="headerlink" title="2.4 梯度下降的算法调优"></a>2.4 梯度下降的算法调优</h3><p>在使用梯度下降时，需要进行调优。哪些地方需要调优呢？</p>
<ol>
<li><p><strong>算法的步长选择</strong>。在前面的算法描述中，提到取步长为1，但是实际上取值取决于数据样本，可以多取一些值，从大到小，分别运行算法，看看迭代效果，如果损失函数在变小，说明取值有效，否则要增大步长。前面说了。步长太大，会导致迭代过快，甚至有可能错过最优解。步长太小，迭代速度太慢，很长时间算法都不能结束。所以算法的步长需要多次运行后才能得到一个较为优的值。</p>
</li>
<li><p><strong>算法参数的初始值选择</strong>。 初始值不同，获得的最小值也有可能不同，因此梯度下降求得的只是局部最小值；当然如果损失函数是凸函数则一定是最优解。由于有局部最优解的风险，需要多次用不同初始值运行算法，关键损失函数的最小值，选择损失函数最小化的初值。</p>
</li>
<li><p><strong>归一化</strong>。由于样本不同特征的取值范围不一样，可能导致迭代很慢，为了减少特征取值的影响，可以对特征数据归一化，也就是对于每个特征$x$，求出它的期望$\overline{x}$和标准差$\operatorname{std}(x)$，然后转化为：</p>
<p> $$<br> \frac{x-\overline{x}}{\operatorname{std}(x)}<br> $$<br>　　这样特征的新期望为0，新方差为1，迭代速度可以大大加快。</p>
</li>
</ol>
<h2 id="3-三大梯度下降优化框架"><a href="#3-三大梯度下降优化框架" class="headerlink" title="3 三大梯度下降优化框架"></a>3 三大梯度下降优化框架</h2><p>有三种梯度下降算法框架，它们不同之处在于每次学习(更新模型参数)使用的<strong>样本个数</strong>或者每次更新使用<strong>不同的样本</strong>会导致每次学习的<strong>准确性</strong>和<strong>学习时间</strong>不同， 下面一一介绍。</p>
<h3 id="3-1-批量梯度下降法（Batch-Gradient-Descent-BGD）"><a href="#3-1-批量梯度下降法（Batch-Gradient-Descent-BGD）" class="headerlink" title="3.1 批量梯度下降法（Batch Gradient Descent, BGD）"></a>3.1 批量梯度下降法（Batch Gradient Descent, BGD）</h3><p>批量梯度下降法也称作为<code>全量梯度下降</code>, 是梯度下降法最常用的形式，从名字就可以看出这个框架每次使用全量的训练集样本来更新模型参数，其公式为：</p>
<p>$$<br>\theta_i \leftarrow \theta_i - \alpha \frac{1}{m} \sum_{j = 0}^{m}  (h_\theta(x_0^j, x_1^j, \ldots, x_n^j) - y^j)x_i^j<br>$$<br>由于我们有m个样本，这里求梯度的时候就用了所有m个样本的梯度数据。</p>
<p>实现代码如下：</p>
<pre class=" language-python"><code class="language-python"><span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span>epochs<span class="token punctuation">)</span><span class="token punctuation">:</span>
    params_grad <span class="token operator">=</span> evaluate_gradient<span class="token punctuation">(</span>loss_function<span class="token punctuation">,</span>data<span class="token punctuation">,</span>params<span class="token punctuation">)</span>
    params <span class="token operator">=</span> params <span class="token operator">-</span> learning_rate <span class="token operator">*</span> params_grad</code></pre>
<p><code>epochs</code> 是用户输入的最大迭代次数。通过上诉代码可以看出，每次使用全部训练集样本计算损失函数<code>loss_function</code>的梯度<code>params_grad</code>，然后使用学习速率<code>learning_rate</code>朝着梯度相反方向去更新模型的每个参数<code>params</code>。一般各现有的一些机器学习库都提供了梯度计算<code>api</code>。如果想自己亲手写代码计算，那么需要在程序调试过程中验证梯度计算是否正确，具体验证方法可以参见<a href="http://cs231n.github.io/neural-networks-3/" target="_blank" rel="noopener">这里</a>。</p>
<p>全量梯度下降每次学习都使用整个训练集，因此其优点在于每次更新都会朝着正确的方向进行，最后能够保证收敛于极值点(凸函数收敛于全局极值点，非凸函数可能会收敛于局部极值点)，但是其缺点在于每次学习时间过长，并且如果训练集很大以至于需要消耗大量的内存，并且全量梯度下降不能进行在线模型参数更新。</p>
<h3 id="3-2-随机梯度下降法（Stochastic-Gradient-Descent-SGD）"><a href="#3-2-随机梯度下降法（Stochastic-Gradient-Descent-SGD）" class="headerlink" title="3.2 随机梯度下降法（Stochastic Gradient Descent, SGD）"></a>3.2 随机梯度下降法（Stochastic Gradient Descent, SGD）</h3><p><code>随机梯度下降法</code>，其实和<code>批量梯度下降法</code>原理类似，区别在与求梯度时没有用所有的m个样本的数据，而是仅仅选取一个样本j来求梯度。对应的更新公式是：</p>
<p>$$<br>\theta_i \leftarrow \theta_i - \alpha (h_\theta(x_0^j, x_1^j, \ldots, x_n^j) - y^j)x_i^j<br>$$</p>
<p>实现代码如下：</p>
<pre class=" language-python"><code class="language-python"><span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span>epochs<span class="token punctuation">)</span><span class="token punctuation">:</span>
    np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>shuffle<span class="token punctuation">(</span>data<span class="token punctuation">)</span>
    <span class="token keyword">for</span> example <span class="token keyword">in</span> data<span class="token punctuation">:</span>
        params_grad <span class="token operator">=</span> evaluate_gradient<span class="token punctuation">(</span>loss_function<span class="token punctuation">,</span>example<span class="token punctuation">,</span>params<span class="token punctuation">)</span>
        params <span class="token operator">=</span> params <span class="token operator">-</span> learning_rate <span class="token operator">*</span> params_grad</code></pre>
<p><code>批量梯度下降算法</code>每次都会使用<strong>全部训练样本</strong>，因此这些计算是冗余的，因为每次都使用完全相同的样本集。而<code>随机梯度下降算法</code>每次只随机选择一个样本来更新模型参数，因此每次的学习是非常快速的，并且可以进行在线更新。</p>
<p>随机梯度下降最大的缺点在于每次更新可能并不会按照正确的方向进行，因此可以带来优化波动(扰动)，如下图：<br><img src="Stogra.png" alt="SGD优化波动"></p>
<p>不过从另一个方面来看，随机梯度下降所带来的波动有个好处就是，对于类似盆地区域（即很多局部极小值点）那么这个波动的特点可能会使得优化的方向从当前的局部极小值点跳到另一个更好的局部极小值点，这样便可能对于非凸函数，最终收敛于一个较好的局部极值点，甚至全局极值点。</p>
<p>由于波动，因此会使得迭代次数（学习次数）增多，即收敛速度变慢。不过最终其会和全量梯度下降算法一样，具有相同的收敛性，即凸函数收敛于全局极值点，非凸损失函数收敛于局部极值点。</p>
<p>随机梯度下降法，和3.1的批量梯度下降法是两个极端，一个采用所有数据来梯度下降，一个用一个样本来梯度下降。自然各自的优缺点都非常突出。对于训练速度来说，随机梯度下降法由于每次仅仅采用一个样本来迭代，训练速度很快，而批量梯度下降法在样本量很大的时候，训练速度不能让人满意。对于准确度来说，随机梯度下降法用于仅仅用一个样本决定梯度方向，导致解很有可能不是最优。对于收敛速度来说，由于随机梯度下降法一次迭代一个样本，导致迭代方向变化很大，不能很快的收敛到局部最优解。</p>
<p>那么，有没有一个中庸的办法能够结合两种方法的优点呢？有！这就是3.3的小批量梯度下降法。</p>
<h3 id="3-3-小批量梯度下降法（Mini-batch-Gradient-Descent-MBGD）"><a href="#3-3-小批量梯度下降法（Mini-batch-Gradient-Descent-MBGD）" class="headerlink" title="3.3 小批量梯度下降法（Mini-batch Gradient Descent, MBGD）"></a>3.3 小批量梯度下降法（Mini-batch Gradient Descent, MBGD）</h3><p>小批量梯度下降法是批量梯度下降法和随机梯度下降法的折衷，也就是对于$m$个样本，我们采用$x$个子样本来迭代，$1&lt;x&lt;m$。一般可以取$x=10$，当然根据样本的数据，可以调整这个$x$的值。对应的更新公式是：</p>
<p>$$<br>\theta_i \leftarrow \theta_i - \alpha \frac{1}{m} \sum_{j = t}^{t+x-1}  (h_\theta(x_0^j, x_1^j, \ldots, x_n^j) - y^j)x_i^j<br>$$</p>
<p>实现代码如下：</p>
<pre class=" language-python"><code class="language-python"><span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span>epochs<span class="token punctuation">)</span><span class="token punctuation">:</span>
    np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>shuffle<span class="token punctuation">(</span>data<span class="token punctuation">)</span>
    <span class="token keyword">for</span> batch <span class="token keyword">in</span> get_batches<span class="token punctuation">(</span>data<span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token number">50</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        params_grad <span class="token operator">=</span> evaluate_gradient<span class="token punctuation">(</span>loss_function<span class="token punctuation">,</span>batch<span class="token punctuation">,</span>params<span class="token punctuation">)</span>
        params <span class="token operator">=</span> params <span class="token operator">-</span> learning_rate <span class="token operator">*</span> params_grad</code></pre>
<p>相对于随机梯度下降，<code>Mini-batch梯度下降</code>降低了<strong>收敛波动性</strong>，即降低了<strong>参数更新的方差</strong>，使得更新<strong>更加稳定</strong>。相对于`全量梯度下降·，其提高了每次<strong>学习的速度</strong>。并且其不用担心内存瓶颈从而可以利用矩阵运算进行高效计算。实践中可以进行多次试验，选择一个更新速度与更次次数都较适合的样本数。</p>
<p><code>Mini-batch梯度下降</code>虽然可以保证收敛性。<code>Mini-batch梯度下降</code>常用于神经网络中。</p>
<h3 id="3-4-面临的挑战"><a href="#3-4-面临的挑战" class="headerlink" title="3.4 面临的挑战"></a>3.4 面临的挑战</h3><p>虽然梯度下降算法效果很好，并且广泛使用，但同时其也存在一些挑战与问题需要解决：</p>
<ol>
<li>选择一个合理的学习速率很难。如果学习速率过小，则会导致收敛速度很慢。如果学习速率过大，那么其会阻碍收敛，即在极值点附近会振荡。</li>
<li>学习速率调整(又称学习速率调度，Learning rate schedules)试图在每次更新过程中，改变学习速率，如退火。一般使用某种事先设定的策略或者在每次迭代中衰减一个较小的阈值。无论哪种调整方法，都需要事先进行固定设置，这边便无法自适应每次学习的数据集特点。</li>
<li>模型所有的参数每次更新都是使用相同的学习速率。如果数据特征是稀疏的或者每个特征有着不同的取值统计特征与空间，那么便不能在每次更新中每个参数使用相同的学习速率，那些很少出现的特征应该使用一个相对较大的学习速率。</li>
<li>对于非凸目标函数，容易陷入那些次优的局部极值点中，如在神经网络中。<a href="http://arxiv.org/abs/1406.2572" target="_blank" rel="noopener">Dauphin</a>指出更严重的问题不是局部极值点，而是鞍点，这些鞍点通常被相同误差的平稳段包围，这使SGD很难逃脱，因为在所有维度上梯度都接近于零。</li>
</ol>
<h2 id="4-梯度下降优化算法"><a href="#4-梯度下降优化算法" class="headerlink" title="4 梯度下降优化算法"></a>4 梯度下降优化算法</h2><p>下面将讨论一些在深度学习社区中经常使用用来解决上诉问题的一些梯度优化方法，不过并不包括在高维数据中不可行的算法，如<a href="https://en.wikipedia.org/wiki/Newton%27s_method_in_optimization" target="_blank" rel="noopener">牛顿法</a>。</p>
<h3 id="4-1-Momentum"><a href="#4-1-Momentum" class="headerlink" title="4.1 Momentum"></a>4.1 Momentum</h3><p>如果在峡谷地区(某些方向较另一些方向上陡峭得多，常见于局部极值点), SGD会在这些地方附近振荡，从而导致收敛速度慢。这种情况下，动量(Momentum)便可以解决。动量在参数更新项中加上一次更新量(即动量项),即：<br>$$<br>\begin{split}<br>v_t &amp;\leftarrow \beta v_{t-1} + \alpha \nabla_{\theta}J(\theta) \\\<br>\theta_t &amp; \leftarrow \theta_{t-1} - v_t<br>\end{split}<br>$$</p>
<p>其中动量项超参数$\gamma &lt; 1$一般是小于等于0.9。</p>
<p>下面两个对比图将体现动量的作用</p>
<p><img src="without_momentum.gif" alt="无动量"><br><img src="with_momentum.gif" alt="加上动量"></p>
<p>加上动量项就像从山顶滚下一个球，求往下滚的时候累积了前面的动量(动量不断增加)，因此速度变得越来越快，直到到达终点。同理，在更新模型参数时，对于那些当前的梯度方向与上一次梯度方向相同的参数，那么进行加强，即这些方向上更快了；对于那些当前的梯度方向与上一次梯度方向不同的参数，那么进行削减，即这些方向上减慢了。因此可以获得更快的收敛速度与减少振荡。</p>
<p>如果需要进一步理解可以参考<a href="https://wangcongying.com/2019/10/08/GD-OptimizationAlgorithms-Momentum/">最优化算法之Momentum</a></p>
<h3 id="4-2-Nesterov-accelerated-gradient-NAG"><a href="#4-2-Nesterov-accelerated-gradient-NAG" class="headerlink" title="4.2 Nesterov accelerated gradient (NAG)"></a>4.2 Nesterov accelerated gradient (NAG)</h3><h3 id="4-3-Adagrad"><a href="#4-3-Adagrad" class="headerlink" title="4.3 Adagrad"></a>4.3 Adagrad</h3><h3 id="4-4-Adadelta"><a href="#4-4-Adadelta" class="headerlink" title="4.4 Adadelta"></a>4.4 Adadelta</h3><h3 id="4-5-RMSprop"><a href="#4-5-RMSprop" class="headerlink" title="4.5 RMSprop"></a>4.5 RMSprop</h3><h3 id="4-6-Adam"><a href="#4-6-Adam" class="headerlink" title="4.6 Adam"></a>4.6 Adam</h3><h3 id="4-7-AdaMax"><a href="#4-7-AdaMax" class="headerlink" title="4.7 AdaMax"></a>4.7 AdaMax</h3><h3 id="4-8-Nadam"><a href="#4-8-Nadam" class="headerlink" title="4.8 Nadam"></a>4.8 Nadam</h3><h3 id="4-9-AMSGrad"><a href="#4-9-AMSGrad" class="headerlink" title="4.9 AMSGrad"></a>4.9 AMSGrad</h3><h3 id="4-10-算法可视化比较"><a href="#4-10-算法可视化比较" class="headerlink" title="4.10 算法可视化比较"></a>4.10 算法可视化比较</h3><h3 id="4-11-算法选择"><a href="#4-11-算法选择" class="headerlink" title="4.11 算法选择"></a>4.11 算法选择</h3><h2 id="5-并行与分布式SGD"><a href="#5-并行与分布式SGD" class="headerlink" title="5 并行与分布式SGD"></a>5 并行与分布式SGD</h2><h3 id="5-1-Hogwild"><a href="#5-1-Hogwild" class="headerlink" title="5.1 Hogwild"></a>5.1 Hogwild</h3><h3 id="5-2-Downpour-SGD"><a href="#5-2-Downpour-SGD" class="headerlink" title="5.2 Downpour SGD"></a>5.2 Downpour SGD</h3><h3 id="5-3-Delay-tolerant-Algorithms-for-SGD"><a href="#5-3-Delay-tolerant-Algorithms-for-SGD" class="headerlink" title="5.3 Delay-tolerant Algorithms for SGD"></a>5.3 Delay-tolerant Algorithms for SGD</h3><h3 id="5-4-TensorFlow"><a href="#5-4-TensorFlow" class="headerlink" title="5.4 TensorFlow"></a>5.4 TensorFlow</h3><h3 id="5-5-Elastic-Averaging-SGD"><a href="#5-5-Elastic-Averaging-SGD" class="headerlink" title="5.5 Elastic Averaging SGD"></a>5.5 Elastic Averaging SGD</h3><h2 id="6-更多SGD优化策略"><a href="#6-更多SGD优化策略" class="headerlink" title="6 更多SGD优化策略"></a>6 更多SGD优化策略</h2><h3 id="6-1-Shuffling-and-Curriculum-Learning"><a href="#6-1-Shuffling-and-Curriculum-Learning" class="headerlink" title="6.1 Shuffling and Curriculum Learning"></a>6.1 Shuffling and Curriculum Learning</h3><h3 id="6-2-Batch-normalization"><a href="#6-2-Batch-normalization" class="headerlink" title="6.2 Batch normalization"></a>6.2 Batch normalization</h3><h3 id="6-3-Early-stopping"><a href="#6-3-Early-stopping" class="headerlink" title="6.3 Early stopping"></a>6.3 Early stopping</h3><h3 id="6-4-Gradient-noise"><a href="#6-4-Gradient-noise" class="headerlink" title="6.4 Gradient noise"></a>6.4 Gradient noise</h3><h2 id="7-梯度下降法和其他无约束优化算法的比较"><a href="#7-梯度下降法和其他无约束优化算法的比较" class="headerlink" title="7 梯度下降法和其他无约束优化算法的比较"></a>7 梯度下降法和其他无约束优化算法的比较</h2><h2 id="8-总结"><a href="#8-总结" class="headerlink" title="8 总结"></a>8 总结</h2><h2 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h2><ol>
<li><a href="http://ruder.io/optimizing-gradient-descent/index.html" target="_blank" rel="noopener">An overview of gradient descent optimization algorithms</a></li>
<li><a href="https://www.cnblogs.com/pinard/p/5970503.html" target="_blank" rel="noopener">刘建平Pinard - 梯度下降（Gradient Descent）小结</a></li>
<li><a href="http://liuchengxu.org/blog-cn/posts/dive-into-gradient-decent/" target="_blank" rel="noopener">理解梯度下降</a></li>
<li><a href="https://blog.csdn.net/walilk/article/details/50978864" target="_blank" rel="noopener">ML重要概念：梯度（Gradient）与梯度下降法（Gradient Descent）</a></li>
<li><a href="https://zh.wikipedia.org/wiki/%E5%AF%BC%E6%95%B0" target="_blank" rel="noopener">维基百科-导数</a></li>
<li><a href="https://zh.wikipedia.org/wiki/偏导数" target="_blank" rel="noopener">维基百科-偏导数</a></li>
<li><a href="https://zh.wikipedia.org/wiki/%E6%96%B9%E5%90%91%E5%AF%BC%E6%95%B0" target="_blank" rel="noopener">维基百科-方向导数</a></li>
<li><a href="https://en.wikipedia.org/wiki/Gradient_descent" target="_blank" rel="noopener">wikipedia-Gradient_descent</a></li>
<li><a href="http://cs231n.github.io/neural-networks-3/" target="_blank" rel="noopener">CS231n Convolutional Neural Networks for Visual Recognition</a></li>
<li><a href="https://en.wikipedia.org/wiki/Matrix_calculus" target="_blank" rel="noopener">wikipedia-矩阵微分 Matrix Calculus</a></li>
<li><a href="https://blog.csdn.net/heyongluoyao8/article/details/52478715" target="_blank" rel="noopener">梯度下降优化算法综述</a></li>
<li><a href="https://arxiv.org/abs/1406.2572" target="_blank" rel="noopener">Identifying and attacking the saddle point problem in high-dimensional non-convex optimization</a></li>
<li><a href="http://incompleteideas.net/papers/sutton-86.pdf" target="_blank" rel="noopener">Two problems with backpropagation and other steepest-descent learning procedures for networks</a></li>
<li></li>
</ol>
]]></content>
      <categories>
        <category>人工智能</category>
      </categories>
      <tags>
        <tag>最优化算法</tag>
      </tags>
  </entry>
  <entry>
    <title>Hexo Matery 常见问题及解决</title>
    <url>/2019/09/25/hexoMateryProblems/</url>
    <content><![CDATA[<h2 id="0-前言"><a href="#0-前言" class="headerlink" title="0 前言"></a>0 前言</h2><p>本文章非 <code>Hexo + Matery Theme</code> 教程，详细教程见<a href="https://github.com/blinkfox/hexo-theme-matery/blob/develop/README_CN.md" target="_blank" rel="noopener">blinkfox hexo-theme-matery</a>，主要记录一些使用中的问题，希望能帮到进来的同学，避免踩坑。</p>
<h2 id="1-常见问题"><a href="#1-常见问题" class="headerlink" title="1 常见问题"></a>1 常见问题</h2><h3 id="1-1-代码相关"><a href="#1-1-代码相关" class="headerlink" title="1.1 代码相关"></a>1.1 代码相关</h3><ol>
<li>在配置了代码高亮后，代码出现无高亮并且布局混乱?<br>首先使用<code>npm</code>安装<code>hexo-prism-plugin</code></li>
</ol>
<pre class=" language-bash"><code class="language-bash"><span class="token function">npm</span> i -S hexo-prism-plugin</code></pre>
<p>然后，修改 Hexo 根目录下 <code>_config.yml</code> 文件中 <code>highlight.enable</code> 的值为 <code>false</code>，并新增 <code>prism</code> 插件相关的配置，主要配置如下：</p>
<pre class=" language-yaml"><code class="language-yaml"><span class="token key atrule">highlight</span><span class="token punctuation">:</span>
  <span class="token key atrule">enable</span><span class="token punctuation">:</span> <span class="token boolean important">false</span>
  <span class="token key atrule">line_number</span><span class="token punctuation">:</span> <span class="token boolean important">true</span>
  <span class="token key atrule">auto_detect</span><span class="token punctuation">:</span> <span class="token boolean important">true</span>
  <span class="token key atrule">tab_replace</span><span class="token punctuation">:</span>

<span class="token key atrule">prism_plugin</span><span class="token punctuation">:</span>
  <span class="token key atrule">mode</span><span class="token punctuation">:</span> <span class="token string">'preprocess'</span>    <span class="token comment" spellcheck="true"># realtime/preprocess</span>
  <span class="token key atrule">theme</span><span class="token punctuation">:</span> <span class="token string">'tomorrow'</span>
  <span class="token key atrule">line_number</span><span class="token punctuation">:</span> <span class="token boolean important">false    </span><span class="token comment" spellcheck="true"># default false</span>
  custom_css<span class="token punctuation">:</span></code></pre>
<h3 id="1-2-Gitalk"><a href="#1-2-Gitalk" class="headerlink" title="1.2 Gitalk"></a>1.2 Gitalk</h3><ol>
<li><code>Gitalk</code> 配置了无效<br>多半是配置的问题，我的问题主要是<code>owen</code>和<code>admin</code>写错了，配置解释如下<pre class=" language-yaml"><code class="language-yaml"><span class="token comment" spellcheck="true"># the Gitalk config，default disabled</span>
<span class="token comment" spellcheck="true"># Gitalk 评论模块的配置，默认为不激活</span>
<span class="token key atrule">gitalk</span><span class="token punctuation">:</span>
<span class="token key atrule">enable</span><span class="token punctuation">:</span> <span class="token boolean important">true</span>
<span class="token key atrule">owner</span><span class="token punctuation">:</span> 
<span class="token key atrule">repo</span><span class="token punctuation">:</span> 
<span class="token key atrule">oauth</span><span class="token punctuation">:</span>
 <span class="token key atrule">clientId</span><span class="token punctuation">:</span> 
 <span class="token key atrule">clientSecret</span><span class="token punctuation">:</span>
<span class="token key atrule">admin</span><span class="token punctuation">:</span> </code></pre>
</li>
</ol>
<ul>
<li><code>enable</code>: <code>gitalk</code> 是否激活</li>
<li><code>owner</code>: 需要填写你<code>Github</code>的用户名，比如我的<code>https://github.com/Congying-Wang/wangcongying.github.io</code>，这里就填写<code>Congying-Wang</code></li>
<li><code>repo</code>: 需要填写你这个仓库的名称，比如我的<code>https://github.com/Congying-Wang/wangcongying.github.io</code>，这里就填写 <code>wangcongying.github.io</code></li>
<li><code>oauth</code>: 这里的<code>clientId</code> 以及 <code>clientSecret</code> 是需要进行创建<code>OAuth APP</code>获取，创建路径: <a href="https://github.com/settings/applications/new，填写格式以及获取路径如下" target="_blank" rel="noopener">https://github.com/settings/applications/new，填写格式以及获取路径如下</a><br><img src="gitalk_id_aply.png" alt="申请填写示范"><br><img src="gitalk_id_pass.png" alt="clientId, clientSecret 获取"></li>
<li><code>admin</code>: 同样需要填写你<code>Github</code>的用户名</li>
</ul>
<h3 id="1-3-数学公式相关"><a href="#1-3-数学公式相关" class="headerlink" title="1.3 数学公式相关"></a>1.3 数学公式相关</h3><ol>
<li>Hexo 不换行<br>在我们使用数学公式的时候使用<code>\\</code>就可以换行，但是在hexo中却换不了，可以使用<code>\\\\</code>来替换 <code>\\</code>即可解决</li>
</ol>
<h3 id="1-4-本地搜索相关"><a href="#1-4-本地搜索相关" class="headerlink" title="1.4 本地搜索相关"></a>1.4 本地搜索相关</h3><ol>
<li><p>本地搜索无效，无结果<br> step1: 运行下面命令安装 <code>searchdb</code></p>
<pre><code> sudo npm install hexo-generator-searchdb --save</code></pre><p> step2: 在<code>themes/hexo-theme-matery/_config.yml</code>中添加下面配置</p>
<pre><code> search:
   path: search.xml
   field: post</code></pre></li>
<li><p>本地搜索框导向了<code>github</code><br> 问题出现在这两个按钮重合在一起了，只要改下一样式就可以，解决方法有多种，自己可以用检查尝试，我是改的<code>.//themes/hexo-theme-matery/source/css/matery.css</code> 样式文件中的<code>.head-container</code>,如下：</p>
<pre class=" language-css"><code class="language-css"> <span class="token selector"><span class="token class">.head-container</span> </span><span class="token punctuation">{</span>
     <span class="token property">position</span><span class="token punctuation">:</span> absolute<span class="token punctuation">;</span>
     <span class="token property">padding</span><span class="token punctuation">:</span> <span class="token number">0</span>px <span class="token number">30</span>px<span class="token punctuation">;</span>
     <span class="token property">width</span><span class="token punctuation">:</span> <span class="token number">100%</span><span class="token punctuation">;</span>
 <span class="token punctuation">}</span></code></pre>
<p> 将其改为：</p>
<pre class=" language-css"><code class="language-css"> <span class="token selector"><span class="token class">.head-container</span> </span><span class="token punctuation">{</span>
     <span class="token property">position</span><span class="token punctuation">:</span> absolute<span class="token punctuation">;</span>
     <span class="token property">padding</span><span class="token punctuation">:</span> <span class="token number">0</span>px <span class="token number">30</span>px<span class="token punctuation">;</span>
     <span class="token property">width</span><span class="token punctuation">:</span> <span class="token number">98%</span><span class="token punctuation">;</span>
 <span class="token punctuation">}</span></code></pre>
<p> 只改动了一个占比，这样搜索框就可以出来了</p>
</li>
</ol>
]]></content>
      <categories>
        <category>其他</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
        <tag>博客</tag>
      </tags>
  </entry>
  <entry>
    <title>CentOS安装Mpich</title>
    <url>/2019/09/25/installMpichOnCentOS/</url>
    <content><![CDATA[<h2 id="0-官方支持"><a href="#0-官方支持" class="headerlink" title="0 官方支持"></a>0 官方支持</h2><p>在网上找了一堆安装方法，但都以失败告终，所以严格<strong>按照官方文档</strong>进行安装最后成功部署了，以本文做个笔记，以防日后踩坑</p>
<p>官方文档：<a href="https://www.mpich.org/static/downloads/3.2.1/mpich-3.2.1-installguide.pdf" target="_blank" rel="noopener">https://www.mpich.org/static/downloads/3.2.1/mpich-3.2.1-installguide.pdf</a><br>以及入门安装文档: <a href="https://www.mpich.org/static/downloads/3.2.1/mpich-3.2.1-README.txt" target="_blank" rel="noopener">https://www.mpich.org/static/downloads/3.2.1/mpich-3.2.1-README.txt</a></p>
<p>安装参数：</p>
<blockquote>
<p>OS: CentOS 7<br>MPI: MPICH 3.2</p>
</blockquote>
<h2 id="1-安装过程"><a href="#1-安装过程" class="headerlink" title="1 安装过程"></a>1 安装过程</h2><h3 id="1-1-编译器升级"><a href="#1-1-编译器升级" class="headerlink" title="1.1 编译器升级"></a>1.1 编译器升级</h3><p>首先需要安装<code>编译器</code>，推荐使用<code>yum</code>进行安装</p>
<pre class=" language-bash"><code class="language-bash"><span class="token function">sudo</span> yum <span class="token function">install</span> <span class="token function">make</span>                           
<span class="token function">sudo</span> yum <span class="token function">install</span> gcc                                 
<span class="token function">sudo</span> yum isntall gcc-c++</code></pre>
<h3 id="1-2-下载安装包并解压"><a href="#1-2-下载安装包并解压" class="headerlink" title="1.2 下载安装包并解压"></a>1.2 下载安装包并解压</h3><ol>
<li>从<a href="https://www.mpich.org/downloads/" target="_blank" rel="noopener">官网</a>上找到要下载的安装包，我这边用的是<code>mpich-3.2.1 (stable release)</code></li>
</ol>
<pre class=" language-bash"><code class="language-bash"><span class="token function">wget</span> http://www.mpich.org/static/downloads/3.2.1/mpich-3.2.1.tar.gz</code></pre>
<ol start="2">
<li><p>解压</p>
<pre class=" language-bash"><code class="language-bash"><span class="token function">tar</span> xzf mpich-3.2.1.tar.gz</code></pre>
</li>
<li><p>创建安装目录<br>官方建议在<code>/home/[USERNAME 用户名]/</code>（即 <code>~/</code>目录）下创建 <code>mpich-install</code>目录，如下</p>
<pre class=" language-bash"><code class="language-bash"><span class="token function">mkdir</span> ~/mpich-install</code></pre>
</li>
</ol>
<h3 id="1-3-配置安装"><a href="#1-3-配置安装" class="headerlink" title="1.3 配置安装"></a>1.3 配置安装</h3><ol>
<li><p>进入下载mpich解压的目录, <strong>注意：以下操作均在下载目录下进行，并非上一步创建的安装路径</strong></p>
</li>
<li><p>运行<code>configure</code> 命令进行安装</p>
<pre class=" language-bash"><code class="language-bash">./configure --prefix<span class="token operator">=</span>~/mpich-install 2<span class="token operator">></span><span class="token operator">&amp;</span>1 <span class="token operator">|</span> <span class="token function">tee</span> c.txt</code></pre>
<p><a href="https://www.mpich.org/static/downloads/3.2.1/mpich-3.2.1-README.txt" target="_blank" rel="noopener">官方文档</a> 中给出了出错的解决方法，报错建议当第一手资料参考</p>
</li>
<li><p>make 生成</p>
<pre class=" language-bash"><code class="language-bash"><span class="token function">make</span> 2<span class="token operator">></span><span class="token operator">&amp;</span>1 <span class="token operator">|</span> <span class="token function">tee</span> m.txt</code></pre>
<p>这时间应该比较长</p>
</li>
<li><p>install 安装</p>
<pre class=" language-bash"><code class="language-bash"><span class="token function">make</span> <span class="token function">install</span> 2<span class="token operator">></span><span class="token operator">&amp;</span>1 <span class="token operator">|</span> <span class="token function">tee</span> mi.txt</code></pre>
</li>
</ol>
<h3 id="1-4-环境变量配置"><a href="#1-4-环境变量配置" class="headerlink" title="1.4 环境变量配置"></a>1.4 环境变量配置</h3><p>到这一步基本上已经安装好了，但是会发现<code>mpicc</code>、<code>mpiexec</code>等指令依旧无法使用，那是因为没有配置环境变量</p>
<ol>
<li><p>打开<code>~/.bashrc</code>环境变量文件，加入下面变量</p>
<pre class=" language-bash"><code class="language-bash"><span class="token function">export</span> PATH<span class="token operator">=</span>/home/<span class="token punctuation">[</span>USERNAME<span class="token punctuation">]</span>/mpich-install/bin:<span class="token variable">$PATH</span></code></pre>
<p>注意：这里的路径是<strong>安装路径</strong></p>
</li>
<li><p>然后使用下面命令让环境变量生效</p>
<pre><code>source ~/.bashrc</code></pre></li>
</ol>
<h2 id="2-测试"><a href="#2-测试" class="headerlink" title="2 测试"></a>2 测试</h2><ol>
<li><p>现在<code>mpich</code>已经部署好了，可以直接用了，官方给出了测试程序，在解压缩下的<code>examples</code> 目录</p>
<pre class=" language-bash"><code class="language-bash"><span class="token function">cd</span> ./examples
mpiexec -n 8 ./cpi</code></pre>
<p>输出结果：<br><img src="/blog_img/installMpichOnCentOS/mpichInstallRes.png" alt="试例输出结果"></p>
</li>
<li><p>自测程序<br>编辑<code>hello.c</code>文件</p>
</li>
</ol>
<pre class=" language-c"><code class="language-c"><span class="token macro property">#<span class="token directive keyword">include</span> <span class="token string">&lt;mpi.h></span></span>
<span class="token macro property">#<span class="token directive keyword">include</span> <span class="token string">&lt;stdio.h></span></span>
<span class="token macro property">#<span class="token directive keyword">include</span> <span class="token string">&lt;math.h></span></span>

<span class="token keyword">int</span> <span class="token function">main</span> <span class="token punctuation">(</span><span class="token keyword">int</span> argc<span class="token punctuation">,</span> <span class="token keyword">char</span> <span class="token operator">*</span><span class="token operator">*</span>argv<span class="token punctuation">)</span>
<span class="token punctuation">{</span>
  <span class="token keyword">int</span> myid<span class="token punctuation">,</span> numprocs<span class="token punctuation">;</span>
  <span class="token keyword">int</span> namelen<span class="token punctuation">;</span>
  <span class="token keyword">char</span> processor_name<span class="token punctuation">[</span>MPI_MAX_PROCESSOR_NAME<span class="token punctuation">]</span><span class="token punctuation">;</span>

  <span class="token function">MPI_Init</span> <span class="token punctuation">(</span><span class="token operator">&amp;</span>argc<span class="token punctuation">,</span> <span class="token operator">&amp;</span>argv<span class="token punctuation">)</span><span class="token punctuation">;</span>
  <span class="token function">MPI_Comm_rank</span> <span class="token punctuation">(</span>MPI_COMM_WORLD<span class="token punctuation">,</span> <span class="token operator">&amp;</span>myid<span class="token punctuation">)</span><span class="token punctuation">;</span>
  <span class="token function">MPI_Comm_size</span> <span class="token punctuation">(</span>MPI_COMM_WORLD<span class="token punctuation">,</span> <span class="token operator">&amp;</span>numprocs<span class="token punctuation">)</span><span class="token punctuation">;</span>
  <span class="token function">MPI_Get_processor_name</span> <span class="token punctuation">(</span>processor_name<span class="token punctuation">,</span> <span class="token operator">&amp;</span>namelen<span class="token punctuation">)</span><span class="token punctuation">;</span>
  <span class="token function">fprintf</span> <span class="token punctuation">(</span><span class="token constant">stderr</span><span class="token punctuation">,</span> <span class="token string">"Hello World! Process %d of %d on %s\n"</span><span class="token punctuation">,</span> myid<span class="token punctuation">,</span> numprocs<span class="token punctuation">,</span> processor_name<span class="token punctuation">)</span><span class="token punctuation">;</span>
  <span class="token function">MPI_Finalize</span> <span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
  <span class="token keyword">return</span> <span class="token number">0</span><span class="token punctuation">;</span>
<span class="token punctuation">}</span></code></pre>
<p>使用以下命令运行</p>
<pre class=" language-bash"><code class="language-bash">mpicc -o hello hello.c        <span class="token comment" spellcheck="true"># 编译 如果`c++`使用`mpicxx`</span>
mpirun -np 4 ./hello</code></pre>
]]></content>
      <categories>
        <category>并行计算</category>
      </categories>
      <tags>
        <tag>Mpich</tag>
      </tags>
  </entry>
  <entry>
    <title>CentOS安装Redis</title>
    <url>/2019/09/23/installRedisOnCentOS/</url>
    <content><![CDATA[<h2 id="1-更新yum源"><a href="#1-更新yum源" class="headerlink" title="1 更新yum源"></a>1 更新yum源</h2><pre class=" language-bash"><code class="language-bash"><span class="token function">sudo</span> yum <span class="token function">install</span> epel-release
<span class="token function">sudo</span> yum update</code></pre>
<blockquote>
<p>注：update 过程中切勿粗暴终止，可能会有造成命令失效从而重装系统的风险</p>
</blockquote>
<h2 id="2-安装Redis数据库并重启"><a href="#2-安装Redis数据库并重启" class="headerlink" title="2 安装Redis数据库并重启"></a>2 安装Redis数据库并重启</h2><pre class=" language-bash"><code class="language-bash"><span class="token function">sudo</span> yum -y <span class="token function">install</span> redis
<span class="token function">sudo</span> systemctl start redis</code></pre>
<p>此时就可以通过 <code>redis-cli</code> 进入命令行模式客户端了</p>
<h2 id="3-配置远程访问"><a href="#3-配置远程访问" class="headerlink" title="3 配置远程访问"></a>3 配置远程访问</h2><p>首先修改配置文件<code>/etc/redis.conf</code>，注意这里需要使用<code>root</code>权限</p>
<ol>
<li><p>修改连接端口<br>修改前：</p>
<pre class=" language-txt"><code class="language-txt">bind 127.0.0.1</code></pre>
<p>修改后</p>
<pre class=" language-txt"><code class="language-txt"># bind 127.0.0.1
bind 0.0.0.0</code></pre>
<blockquote>
<p>注：这里有两种方式，一可以直接注释<code>bind 127.0.0.1</code>, 也可以改成<code>bind 0.0.0.0</code></p>
</blockquote>
</li>
<li><p>配置连接密码<br>修改前：</p>
<pre class=" language-txt"><code class="language-txt">#requirepass foobared</code></pre>
<p>修改后</p>
<pre class=" language-txt"><code class="language-txt">requirepass [你的密码]</code></pre>
</li>
</ol>
<p>然后保存，重启redis 服务</p>
<pre class=" language-bash"><code class="language-bash"><span class="token function">sudo</span> systemctl restart redis</code></pre>
<h2 id="4-常见Redis服务命令"><a href="#4-常见Redis服务命令" class="headerlink" title="4 常见Redis服务命令"></a>4 常见Redis服务命令</h2><pre class=" language-bash"><code class="language-bash">systemctl start redis.service     <span class="token comment" spellcheck="true"># 启动redis服务器</span>

systemctl stop redis.service     <span class="token comment" spellcheck="true"># 停止redis服务器</span>

systemctl restart redis.service    <span class="token comment" spellcheck="true"># 重新启动redis服务器</span>

systemctl status redis.service     <span class="token comment" spellcheck="true"># 获取redis服务器的运行状态</span>

systemctl <span class="token function">enable</span> redis.service    <span class="token comment" spellcheck="true"># 开机启动redis服务器</span>

systemctl disable redis.service    <span class="token comment" spellcheck="true"># 开机禁用redis服务器</span></code></pre>
<h2 id="5-常见问题"><a href="#5-常见问题" class="headerlink" title="5 常见问题"></a>5 常见问题</h2><p><strong>Q1</strong>: 使用其他局域网内机器访问不了？<br><strong>A1</strong>: 一般是防火墙问题，现把<code>6379</code>端口开放，然后重启防火墙即可，操作命令个如下</p>
<pre class=" language-bash"><code class="language-bash">firewall-cmd --permanent --add-port<span class="token operator">=</span>6379/tcp
firewall-cmd --reload</code></pre>
<p><strong>Q2</strong>: 局域网内其他机器访问<br><strong>A2</strong>: 使用<code>redis-cli</code> 命令</p>
<pre class=" language-bash"><code class="language-bash">redis-cli -h <span class="token punctuation">[</span>host<span class="token punctuation">]</span> -p <span class="token punctuation">[</span>port<span class="token punctuation">]</span> -a <span class="token punctuation">[</span>password<span class="token punctuation">]</span></code></pre>
]]></content>
      <categories>
        <category>SRE运维</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title>Dart 中 final 和 const 区别</title>
    <url>/2019/09/20/distinguishBetweenFinalAndConstOnDart/</url>
    <content><![CDATA[<p>用<code>final</code>修饰的变量，必须在定义时将其初始化，其值在初始化后不可改变；<br><code>const</code>用来定义常量。</p>
<p>它们的区别在于，<code>const</code>比<code>final</code>更加严格。<code>final</code>只是要求变量在初始化后值不变，但通过<code>final</code>，我们无法在编译时（运行之前）知道这个变量的值；而<code>const</code>所修饰的是编译时常量，我们在编译时就已经知道了它的值，显然，它的值也是不可改变的。</p>
<p>下面先用简单的例子说明一下区别，再通过代码理解”<code>const</code>所修饰的是编译时常量”这句话：</p>
<h2 id="基本用法"><a href="#基本用法" class="headerlink" title="基本用法"></a>基本用法</h2><p><strong>final</strong>：只能被设一次值，在声明处赋值，值和普通变量的设值一样，可以是对象、字符串、数字等，用于修饰值的表达式不变的变量；</p>
<pre class=" language-java"><code class="language-java"><span class="token keyword">final</span> name <span class="token operator">=</span> <span class="token string">'Bob'</span><span class="token punctuation">;</span>   
<span class="token comment" spellcheck="true">// name = 'job'; //运行出错，因为final修饰的变量不能调用其setter方法，即：不能设值</span></code></pre>
<p><strong>const</strong>：只能被设一次值，在声明处赋值，且值必须为编译时常量；用于修饰常量。</p>
<pre class=" language-java"><code class="language-java"><span class="token keyword">const</span> bar <span class="token operator">=</span> <span class="token number">1000000</span><span class="token punctuation">;</span>       <span class="token comment" spellcheck="true">// 定义常量值</span>
<span class="token comment" spellcheck="true">// bar =13;   // 出现异常，const修饰的变量不能调用setter方法，即：不能设值，只能在声明处设值</span>
<span class="token keyword">const</span> atm <span class="token operator">=</span> <span class="token number">1.01325</span> <span class="token operator">*</span> bar<span class="token punctuation">;</span> <span class="token comment" spellcheck="true">// 值的表达式中的变量必须是编译时常量（bar）;</span>

var c <span class="token operator">=</span> <span class="token number">12</span><span class="token punctuation">;</span>        

<span class="token comment" spellcheck="true">//  atm = 1 * c;  //出错，因为c不是一个编译时常量，即：非const修饰的变量（只有const修饰的变量才是编译时常量）</span></code></pre>
<p><strong>const还可以用来声明常量值:</strong></p>
<pre class=" language-java"><code class="language-java"><span class="token comment" spellcheck="true">// [] 创建一个空列表.</span>
<span class="token comment" spellcheck="true">// const [] 创建一个空的不可变列表 (EIA).</span>
var foo <span class="token operator">=</span> <span class="token keyword">const</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">;</span>   <span class="token comment" spellcheck="true">// foo 目前是一个 EIA.</span>
<span class="token keyword">final</span> bar <span class="token operator">=</span> <span class="token keyword">const</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">;</span> <span class="token comment" spellcheck="true">// bar 永远是一个 EIA.</span>
<span class="token keyword">const</span> baz <span class="token operator">=</span> <span class="token keyword">const</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">;</span> <span class="token comment" spellcheck="true">// baz 是一个编译时常量 EIA.</span>

<span class="token comment" spellcheck="true">//你可以改变 非final, 非const 修饰的变量,</span>
<span class="token comment" spellcheck="true">// 即使它的值为编译时常量值.</span>
foo <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">;</span>

<span class="token comment" spellcheck="true">// 不能改变final和const修饰的变量的值.</span>
<span class="token comment" spellcheck="true">// bar = []; // 未处理的异常.</span>
<span class="token comment" spellcheck="true">// baz = []; // 未处理的异常.</span></code></pre>
<h2 id="高级使用"><a href="#高级使用" class="headerlink" title="高级使用"></a>高级使用</h2><p>当为final修饰的值赋一个包含成员变量或方法的对象时：</p>
<ol>
<li>对象成员值能被修改，对于能够添加成员的类（如List、Map）则可以添加或删除成员</li>
<li>变量本身实例不能被修改</li>
</ol>
<pre class=" language-java"><code class="language-java"><span class="token keyword">class</span> <span class="token class-name">Point</span><span class="token punctuation">{</span>
  var x<span class="token punctuation">,</span>y<span class="token punctuation">;</span>
  <span class="token function">Point</span><span class="token punctuation">(</span><span class="token keyword">this</span><span class="token punctuation">.</span>x<span class="token punctuation">,</span><span class="token keyword">this</span><span class="token punctuation">.</span>y<span class="token punctuation">)</span><span class="token punctuation">{</span>

  <span class="token punctuation">}</span>
<span class="token punctuation">}</span>

<span class="token function">main</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>
   <span class="token keyword">final</span>  p <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">Point</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">;</span> 
   <span class="token comment" spellcheck="true">// p = new Point(3,4);   //出错，final修饰的变量不能调用setter方法；</span>
      p<span class="token punctuation">.</span>x<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">;</span>    <span class="token comment" spellcheck="true">// 正常执行，修改的是变量的属性值，而不是变量引用的对象；</span>
      <span class="token function">print</span><span class="token punctuation">(</span><span class="token string">''</span>p<span class="token punctuation">.</span>x<span class="token punctuation">)</span><span class="token punctuation">;</span>  <span class="token comment" spellcheck="true">// 打印 2</span>


  var foo <span class="token operator">=</span> <span class="token keyword">const</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">;</span>  
    foo <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">;</span>  
    <span class="token comment" spellcheck="true">/*此部分代码的重点在于var foo , 一个正常变量可以随意赋值或更改，重点不在const [],
      所以不要纠结const []是不可变的。[]和[1,2,1]是不同的对象*/</span>
    <span class="token function">print</span><span class="token punctuation">(</span>foo<span class="token punctuation">)</span><span class="token punctuation">;</span>   


 <span class="token keyword">final</span> baz <span class="token operator">=</span>  <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">;</span>
 <span class="token comment" spellcheck="true">// baz=[1,2,3,4]; //出错 此调用修改了变量的实例 即：[1] 和[1,2,3,4]是不同的对象</span>
    baz<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">;</span>     <span class="token comment" spellcheck="true">//正常执行，只修改了变量引用对象的成员变量的值</span>
    <span class="token function">print</span><span class="token punctuation">(</span>baz<span class="token punctuation">)</span><span class="token punctuation">;</span> 

 <span class="token keyword">final</span> bad <span class="token operator">=</span>  <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">;</span>   <span class="token comment" spellcheck="true">//List&lt;int></span>
    bad<span class="token punctuation">.</span><span class="token function">add</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token comment" spellcheck="true">//正常执行，向变量引用对象添加成员</span>
    bad<span class="token punctuation">.</span><span class="token function">add</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">;</span> 
    <span class="token function">print</span><span class="token punctuation">(</span>bad<span class="token punctuation">)</span>

 <span class="token keyword">final</span> Map<span class="token operator">&lt;</span>String<span class="token punctuation">,</span> String<span class="token operator">></span> cache <span class="token operator">=</span> <span class="token operator">&lt;</span>String<span class="token punctuation">,</span> String<span class="token operator">></span><span class="token punctuation">{</span><span class="token punctuation">}</span><span class="token punctuation">;</span>  <span class="token comment" spellcheck="true">//Map</span>
   cache<span class="token punctuation">[</span><span class="token string">'name1'</span><span class="token punctuation">]</span><span class="token operator">=</span><span class="token string">'1213'</span><span class="token punctuation">;</span>
   cache<span class="token punctuation">[</span><span class="token string">'name2'</span><span class="token punctuation">]</span><span class="token operator">=</span><span class="token string">'1313'</span><span class="token punctuation">;</span>
   <span class="token function">print</span><span class="token punctuation">(</span>cache<span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token punctuation">}</span></code></pre>
<h2 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h2><ol>
<li><a href="http://www.voidcn.com/article/p-eqqxibed-bqr.html" target="_blank" rel="noopener">http://www.voidcn.com/article/p-eqqxibed-bqr.html</a></li>
<li><a href="https://my.oschina.net/jthmath/blog/419232" target="_blank" rel="noopener">https://my.oschina.net/jthmath/blog/419232</a></li>
<li><a href="http://dart.goodev.org/guides/language/language-tour" target="_blank" rel="noopener">http://dart.goodev.org/guides/language/language-tour</a></li>
</ol>
]]></content>
      <categories>
        <category>移动开发</category>
      </categories>
      <tags>
        <tag>Dart</tag>
      </tags>
  </entry>
</search>
